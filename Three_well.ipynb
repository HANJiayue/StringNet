{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPf9x7DXPS5z"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtcBG7SL07R8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pickle\n",
        "\n",
        "mycase=\"tw\"\n",
        "casenum=\"6\"\n",
        "\n",
        "\n",
        "all_path=\"/content/drive/MyDrive/Colab_Notebooks/2023_MEP/1107_data/\"\n",
        "\n",
        "path_s=all_path+mycase+\"_s_\"+casenum+\".pkl\"\n",
        "path_path=all_path+mycase+\"_path_\"+casenum+\".pkl\"\n",
        "path_cos=all_path+mycase+\"_cos_\"+casenum+\".pkl\"\n",
        "path_energy=all_path+mycase+\"_energy_\"+casenum+\".pkl\"\n",
        "path_force=all_path+mycase+\"_force_\"+casenum+\".pkl\"\n",
        "path_cosloss=all_path+mycase+\"_coslosss_\"+casenum+\".pkl\"\n",
        "path_lmax=all_path+mycase+\"_lmax_\"+casenum+\".pkl\"\n",
        "path_lg=all_path+mycase+\"_lg_\"+casenum+\".pkl\"\n",
        "\n",
        "plot_use_all=[]\n",
        "\n",
        "torch.set_printoptions(precision=10)\n",
        "\n",
        "model_Parameters_name = all_path+\"model_MEP_\"+mycase[0:2]+\"_\"+casenum+\"pretrain\"+\".pth\" # Path\n",
        "model_name= all_path+\"model_MEP_\"+mycase+\"_\"+casenum+\".pth\"\n",
        "\n",
        "\n",
        "\n",
        "load_model = True\n",
        "batches = 2000\n",
        "beta = 10\n",
        "learning_rate = 1e-4\n",
        "dimension = 2\n",
        "\n",
        "x_lb=-2\n",
        "x_ub=2\n",
        "y_lb=-1.5\n",
        "y_ub=2.5\n",
        "\n",
        "\n",
        "alpha1=1\n",
        "alpha2=10\n",
        "alpha3=1\n",
        "\n",
        "\n",
        "def V_tensor(x):\n",
        "  x_all=torch.cat(x,axis=1)\n",
        "  Z=3*torch.exp(-x_all[:,0]**2-(x_all[:,1]-1/3)**2)-3*torch.exp(-x_all[:,0]**2-(x_all[:,1]-5/3)**2)-5*torch.exp(-(x_all[:,0]-1)**2-x_all[:,1]**2)-5*torch.exp(-(x_all[:,0]+1)**2-x_all[:,1]**2)#+1/5*x_all[:,0]**4+1/5*(x_all[:,1]-1/3)**4\n",
        "  return Z\n",
        "\n",
        "\n",
        "def V_tensor_grad(x): # first order derivative of V w.r.t x\n",
        "    grad_Vx=[]\n",
        "    output = V_tensor(x)\n",
        "    for i in range(dimension):\n",
        "      grad_Vx.append(torch.autograd.grad(outputs=output, inputs=x[i], grad_outputs=torch.ones_like(output), create_graph=True)[0])\n",
        "\n",
        "    return grad_Vx\n",
        "\n",
        "def fig_cos_V_force(s,cos2,g,x_pred_list):\n",
        "    fig = plt.figure(figsize=(15,4))\n",
        "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.plot(s,cos2,'b.')\n",
        "    plt.ylim(-0.1,1.1)\n",
        "    plt.title(\"$Cos^2$\")\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.plot(s,V_tensor(x_pred_list).cpu().detach().numpy(), 'b.')\n",
        "    plt.title(\"$Energy$\")\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "\n",
        "    force=np.sqrt(np.sum(g*g, axis=1))\n",
        "    plt.plot(s,force, 'b.')\n",
        "    plt.title(\"$Force$\")\n",
        "\n",
        "def fig_loss_batch(plt_batch,loss_batch):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.plot(plt_batch,loss_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "def fig_cos(plt_batch,cos_batch,lmax_batch,lg_batch):\n",
        "    fig = plt.figure(figsize=(15,4))\n",
        "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.semilogy(plt_batch,cos_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"$\\int 1-Cos^2(force,tangent) ds$\")\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.plot(plt_batch,lmax_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"$lmax$\")\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.plot(plt_batch,lg_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"$l_g$\")\n",
        "\n",
        "def fig_countour(x_pred):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    x = np.linspace(x_lb, x_ub,num=51,endpoint=True)\n",
        "    y = np.linspace(y_lb, y_ub,num=51,endpoint=True)\n",
        "\n",
        "    X,Y = np.meshgrid(x,y)\n",
        "    X_new=X.reshape(-1,1)\n",
        "    Y_new=Y.reshape(-1,1)\n",
        "    XY=np.hstack((X_new,Y_new))\n",
        "    for i in range(dimension-2):\n",
        "      XY=np.hstack((XY,np.zeros(X_new.shape)))\n",
        "\n",
        "    XY_tensor=torch.from_numpy(XY)\n",
        "    X_list = []\n",
        "    for i in range(dimension):\n",
        "      X_list.append(XY_tensor[:, i:i+1].to(device))\n",
        "    Z = V_tensor(X_list)\n",
        "\n",
        "    Z_new=Z.reshape(X.shape).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "    plt.figure()\n",
        "    CS = plt.contourf(X,Y,Z_new,50,cmap=mpl.cm.jet)\n",
        "    plt.colorbar(CS)\n",
        "    x_plot=x_pred.cpu().detach().numpy()\n",
        "    plt.plot(x_plot[:,0],x_plot[:,1],\"r.\",markersize=5)\n",
        "    plt.xlim((x_lb,x_ub))\n",
        "    plt.ylim((y_lb,y_ub))\n",
        "    plt.xlabel('$x_1$')\n",
        "    plt.ylabel('$x_2$')\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self,st,ed):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_tanh_stack = nn.Sequential(\n",
        "            nn.Linear(1,16),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(16, 32),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(16, dimension)\n",
        "        )\n",
        "        self.startpoint=Variable(torch.from_numpy(np.array([st])),requires_grad=False).to(device)\n",
        "        self.endpoint=Variable(torch.from_numpy(np.array([ed])),requires_grad=False).to(device)\n",
        "\n",
        "\n",
        "    def forward(self, s):\n",
        "        s = self.flatten(s)\n",
        "        x_pred = self.linear_tanh_stack(s)\n",
        "        out=s*(1-s)*x_pred + (1-s)*self.startpoint + s*self.endpoint\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def train(model):\n",
        "    global my_V\n",
        "    cos_batch=[]\n",
        "    loss_batch=[]\n",
        "    plt_batch=[]\n",
        "\n",
        "    lg_batch=[]\n",
        "    lmax_batch=[]\n",
        "    if load_model:\n",
        "        model.load_state_dict(torch.load(model_Parameters_name))\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(batches):\n",
        "        if i<4000:\n",
        "          alpha1=0.1\n",
        "          alpha2=0.1\n",
        "          alpha3=0.0000001\n",
        "        else:\n",
        "          alpha1=0.0001\n",
        "          alpha2=0.1\n",
        "          alpha3=0.0000001\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        s = np.random.uniform(0.001,0.999,(500,1))\n",
        "        s = Variable(torch.from_numpy(s),requires_grad=True).to(device)\n",
        "        x_pred = model(s)\n",
        "        x_pred_list = []\n",
        "        for n in range(dimension):\n",
        "            x_pred_list.append(x_pred[:, n:n+1])\n",
        "\n",
        "        v = V_tensor(x_pred_list).to(device)\n",
        "        gradx_list=[]\n",
        "        gradgx_list=[]\n",
        "        for n in range(dimension):\n",
        "            gradx_list.append(torch.autograd.grad(outputs=x_pred_list[n],inputs=s,grad_outputs=torch.ones_like(s),create_graph=True)[0]) # First order derivative of x w.r.t s\n",
        "            gradgx_list.append(torch.autograd.grad(outputs=gradx_list[n],inputs=s,grad_outputs=torch.ones_like(s),create_graph=True)[0]) # Second order derivative of x w.r.t s\n",
        "\n",
        "        partial_s_x = torch.cat(gradx_list, axis=1)\n",
        "        dot_partial_s_x = torch.sqrt(torch.sum(partial_s_x*partial_s_x, axis=1, keepdim=False)) #s对神经网络的一阶导数的数量积\n",
        "\n",
        "        loss_1=1/beta*torch.log(torch.mean(torch.exp(v*beta)*dot_partial_s_x))\n",
        "\n",
        "        g_all = V_tensor_grad(x_pred_list)\n",
        "        g=torch.cat(g_all, axis=1)\n",
        "        cos2 = (torch.sum(partial_s_x*g,axis=1, keepdim=True)/(torch.sum(g*g, axis=1, keepdim=True)**0.5*torch.sum(partial_s_x*partial_s_x, axis=1, keepdim=True)**0.5))**2\n",
        "        loss_2 = torch.mean(1-cos2)\n",
        "\n",
        "        cons=0\n",
        "        for n in range(dimension):\n",
        "            cons += torch.sum(gradx_list[n]*gradgx_list[n], axis=1, keepdim=True)\n",
        "        loss_3 = torch.mean(cons**2)\n",
        "\n",
        "        loss = alpha1*loss_1 +alpha2*loss_2+alpha3*loss_3\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i) % 50 == 0:\n",
        "            ss = np.linspace(0.001,0.999,1000,endpoint=True)\n",
        "\n",
        "            ss = ss.reshape(-1,1)\n",
        "            ss = Variable(torch.from_numpy(ss),requires_grad=True).to(device)\n",
        "            xx = model(ss)\n",
        "            gradx_list_plot=[]\n",
        "            x_pred_list_plot = []\n",
        "            for n in range(dimension):\n",
        "                x_pred_list_plot.append(xx[:, n:n+1])\n",
        "            for n in range(dimension):\n",
        "                gradx_list_plot.append(torch.autograd.grad(outputs=x_pred_list_plot[n],inputs=ss,grad_outputs=torch.ones_like(ss),create_graph=True)[0]) # First order derivative of x w.r.t s\n",
        "            partial_s_x_plot = torch.cat(gradx_list_plot, axis=1)\n",
        "\n",
        "            g_all_plot = V_tensor_grad(x_pred_list_plot)\n",
        "            g_plot=torch.cat(g_all_plot, axis=1)\n",
        "            cos2_plot = (torch.sum(partial_s_x_plot*g_plot,axis=1, keepdim=True)/(torch.sum(g_plot*g_plot, axis=1, keepdim=True)**0.5*torch.sum(partial_s_x_plot*partial_s_x_plot, axis=1, keepdim=True)**0.5))**2\n",
        "            cos_plot_loss = torch.mean(1-cos2_plot)\n",
        "            cos_batch.append(cos_plot_loss.item())\n",
        "\n",
        "            U_max=torch.max(V_tensor(x_pred_list_plot))\n",
        "            lmax_batch.append(U_max.item())\n",
        "\n",
        "            dot_partial_s_x_plot = torch.sqrt(torch.sum(partial_s_x_plot*partial_s_x_plot, axis=1, keepdim=False))\n",
        "            dot_g_plot= torch.sqrt(torch.sum(g_plot*g_plot, axis=1, keepdim=False))\n",
        "            lg=torch.mean(dot_g_plot*dot_partial_s_x_plot)\n",
        "\n",
        "            lg_batch.append(lg.item())\n",
        "\n",
        "            loss, batch = alpha1*loss_1.item() + alpha2*loss_2.item() + alpha3*loss_3.item(), i\n",
        "            print(f'batches: {batch+1}')\n",
        "            print(f'loss1: {alpha1*loss_1} loss2: {alpha2*loss_2} loss3: {alpha3*loss_3}')\n",
        "\n",
        "            loss_batch.append(loss)\n",
        "            plt_batch.append(i+1)\n",
        "            plt.figure()\n",
        "\n",
        "        if (i+1) % 500 == 0:\n",
        "            print(\"cos_plot_loss.item()=\",cos_plot_loss.item())\n",
        "            fig_cos(plt_batch,cos_batch,lmax_batch,lg_batch)\n",
        "            fig_loss_batch(plt_batch,loss_batch)\n",
        "            fig_cos_V_force(s.cpu().detach().numpy(),cos2.cpu().detach().numpy(),g.cpu().detach().numpy(),x_pred_list)  # Why [1:-1\n",
        "            fig_countour(x_pred)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            torch.save(model.state_dict(), model_name)\n",
        "            print(\"Saved PyTorch Model State to \" +str(model_name))\n",
        "\n",
        "    my_V=V_tensor(x_pred_list_plot)\n",
        "\n",
        "    output = open(path_s, 'wb')\n",
        "    pickle.dump(s.cpu().detach().numpy(),output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_path, 'wb')\n",
        "    pickle.dump(x_pred.cpu().detach().numpy(),output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_cos, 'wb')\n",
        "    pickle.dump(cos2.cpu().detach().numpy(),output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_energy, 'wb')\n",
        "    pickle.dump(V_tensor(x_pred_list).cpu().detach().numpy(),output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_force, 'wb')\n",
        "    pickle.dump(np.sqrt(np.sum(g.cpu().detach().numpy()*g.cpu().detach().numpy(), axis=1)),output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_cosloss, 'wb')\n",
        "    pickle.dump(cos_batch,output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_lg, 'wb')\n",
        "    pickle.dump(lg_batch,output)\n",
        "    output.close()\n",
        "\n",
        "    output = open(path_lmax, 'wb')\n",
        "    pickle.dump(lmax_batch,output)\n",
        "    output.close()\n",
        "\n",
        "    return\n",
        "\n",
        "def train_pre(model):\n",
        "    for i in range(10):\n",
        "      a=np.array([-81.0640995,70.67714575])\n",
        "      b=np.array([69.43047688,-67.71500979])\n",
        "\n",
        "      c=(a+b)/2\n",
        "      r=np.sqrt(np.sum((a-b)**2))/2\n",
        "\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "      t = np.random.uniform(0,1,(500,1))\n",
        "      t = Variable(torch.from_numpy(t),requires_grad=True).to(device)\n",
        "\n",
        "      xy=model(t)\n",
        "      x,y=torch.split(xy,[1,1],dim=1)\n",
        "\n",
        "      loss=torch.mean((x-c[0]-r*torch.cos(np.pi*t+2.398))**2+(y+c[1]+r*torch.sin(np.pi*t+2.398))**2)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if i % 200==0:\n",
        "        print(loss.item())\n",
        "    torch.save(model.state_dict(), model_Parameters_name)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "    MEPpass_start_point=[-1.13366646,-0.03864257]\n",
        "    MEPpass_end_point=[1.13366646,-0.03864257]\n",
        "\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using {device} device to train\")\n",
        "\n",
        "    model = NeuralNetwork(MEPpass_start_point,MEPpass_end_point).to(device)\n",
        "    model = model.double()\n",
        "\n",
        "    model = train_pre(model)\n",
        "    train(model)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}