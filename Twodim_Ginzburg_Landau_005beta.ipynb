{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M7kUw2a_bKa"
      },
      "source": [
        "# Generate minimals (-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnY7SDI6zR07"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pickle\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "dn=21\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device to train\")\n",
        "\n",
        "mycase=\"find_minimals\"\n",
        "casenum=\"m1_k008\"\n",
        "\n",
        "\n",
        "all_path=\"/content/drive/MyDrive/Colab_Notebooks/2023_mep/0120/\"\n",
        "model_Parameters_name = all_path+casenum+\"/\"+\"model_MEP_\"+mycase[0:2]+\"_\"+\"pre2\"+\"pretrain\"+\".pth\" # Path\n",
        "\n",
        "model_name=all_path+casenum+\"/\"+\"model_MEP_\"+mycase+\"_\"+casenum+\".pth\"\n",
        "\n",
        "alpha1=10000\n",
        "alpha5=1\n",
        "kappa=0.08\n",
        "\n",
        "\n",
        "load_model = False\n",
        "batches = 20000\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "dn=31\n",
        "x_dim=2\n",
        "\n",
        "pre_train_step=500\n",
        "\n",
        "\n",
        "def mkdir_pre(path):\n",
        "  folder = os.path.exists(path)\n",
        "  if not folder:\n",
        "    os.makedirs(path)\n",
        "    print(\"---  new folder...  ---\")\n",
        "    print(\"---  OK  ---\")\n",
        "  else:\n",
        "    print(\"---  There is this folder!  ---\")\n",
        "\n",
        "\n",
        "def fig_loss_batch_pre(plt_batch,loss_batch):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.semilogy(plt_batch,loss_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "def fig_cos_pre(plt_batch,lg_batch,j):\n",
        "    fig = plt.figure(figsize=(5,4))\n",
        "    plt.semilogy(plt_batch,lg_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"loss on grid\")\n",
        "    if j == batches-500:\n",
        "      plt.savefig(all_path+casenum+\"/\"+\"lgloss\"+\".jpg\")\n",
        "\n",
        "\n",
        "def fig_countour_pre(xx,phi,j):\n",
        "    xx1=xx[:,0]\n",
        "    xx2=xx[:,1]\n",
        "\n",
        "    xx1=xx1.reshape(dn,dn)\n",
        "    xx2=xx2.reshape(dn,dn)\n",
        "\n",
        "    phi_plot_sq=phi.reshape(dn,dn)\n",
        "\n",
        "\n",
        "    fig2 = plt.figure(figsize=(24,4))\n",
        "    fig2.add_subplot(1,1,1)\n",
        "    picori=phi_plot_sq.reshape(dn,dn)\n",
        "    pic=np.zeros((dn,dn,3))\n",
        "\n",
        "    B=(np.sign(picori)+1)/2*picori\n",
        "    C=(np.sign(picori)-1)/2*picori\n",
        "\n",
        "\n",
        "    pic[:,:,0]=B\n",
        "    pic[:,:,-1]=C\n",
        "\n",
        "    plt.imshow(pic)\n",
        "\n",
        "\n",
        "    plt.savefig(all_path+casenum+\"/\"+str(j)+\".jpg\")\n",
        "\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_tanh_stack = nn.Sequential(\n",
        "            nn.Linear(2,32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, s):\n",
        "        phi_pred = self.linear_tanh_stack(s)\n",
        "        return phi_pred\n",
        "\n",
        "\n",
        "\n",
        "def train(model):\n",
        "    loss_batch=[]\n",
        "    plt_batch=[]\n",
        "    lg_batch=[]\n",
        "\n",
        "    for i in range(batches):\n",
        "\n",
        "        if i<1000:\n",
        "            learning_rate=1e-3\n",
        "        elif i<10000:\n",
        "            learning_rate=1e-4\n",
        "        else:\n",
        "            learning_rate=1e-5\n",
        "\n",
        "        if i<2000:\n",
        "          alpha1=10\n",
        "          alpha5=1\n",
        "        else:\n",
        "          alpha1=10\n",
        "          alpha5=1\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        datanum=500\n",
        "        x_np = np.random.uniform(0.001,0.999,(datanum,2))\n",
        "\n",
        "\n",
        "        x_tensor=Variable(torch.from_numpy(x_np),requires_grad=True).to(device)\n",
        "\n",
        "        x_all=[]\n",
        "        for j in range(x_dim):\n",
        "          x_all.append(x_tensor[:,j:j+1])\n",
        "\n",
        "        x_tensor_all=torch.cat(x_all,1)\n",
        "        phi_int_tensor=model(x_tensor_all)\n",
        "\n",
        "        if i<pre_train_step:\n",
        "          loss_res=torch.mean((phi_int_tensor+1)**2)\n",
        "          bound_samp=100\n",
        "\n",
        "          xlu=np.random.uniform(0.001,0.999,(bound_samp,2))\n",
        "          xlu[:,0]=0.0\n",
        "          xlu_tensor=Variable(torch.from_numpy(xlu),requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "          xru=np.random.uniform(0.001,0.999,(bound_samp,2))\n",
        "          xru[:,0]=1.0\n",
        "          xru_tensor=Variable(torch.from_numpy(xru),requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "          xut=np.random.uniform(0.001,0.999,(bound_samp,2))\n",
        "          xut[:,1]=1.0\n",
        "          xut_tensor=Variable(torch.from_numpy(xut),requires_grad=True).to(device)\n",
        "\n",
        "          xub=np.random.uniform(0.001,0.999,(bound_samp,2))\n",
        "          xub[:,1]=0.0\n",
        "          xub_tensor=Variable(torch.from_numpy(xub),requires_grad=True).to(device)\n",
        "\n",
        "          phi_lu=model(xlu_tensor)\n",
        "          phi_ru=model(xru_tensor)\n",
        "          phi_ut=model(xut_tensor)\n",
        "          phi_ub=model(xub_tensor)\n",
        "\n",
        "          phi_p1_np=1.0*np.ones((bound_samp))\n",
        "          phi_m1_np=-1.0*np.ones((bound_samp))\n",
        "\n",
        "          phi_p1=Variable(torch.from_numpy(phi_p1_np),requires_grad=True).to(device)\n",
        "          phi_m1=Variable(torch.from_numpy(phi_m1_np),requires_grad=True).to(device)\n",
        "\n",
        "          loss_bound=torch.mean((phi_lu.reshape(-1)-phi_p1)**2)+torch.mean((phi_ru.reshape(-1)-phi_p1)**2)+torch.mean((phi_ut.reshape(-1)-phi_m1)**2)+torch.mean((phi_ub.reshape(-1)-phi_m1)**2)\n",
        "\n",
        "          loss=alpha1*loss_res+alpha5*loss_bound\n",
        "\n",
        "        else:\n",
        "          gradx_all=[]\n",
        "          for j in range(x_dim):\n",
        "            gradx_int = torch.autograd.grad(outputs=phi_int_tensor,inputs=x_all[j],grad_outputs=torch.ones_like(x_tensor[:,j:j+1]),create_graph=True)[0]\n",
        "            gradx_all.append(gradx_int)\n",
        "\n",
        "          gradx2_int=0.0\n",
        "          for j in range(x_dim):\n",
        "            gradx2_j=torch.autograd.grad(outputs=gradx_all[j],inputs=x_all[j],grad_outputs=torch.ones_like(x_tensor[:,j:j+1]),create_graph=True)[0]\n",
        "            gradx2_int=gradx2_int+gradx2_j\n",
        "\n",
        "\n",
        "          loss_res=torch.mean((kappa**2*gradx2_int-(phi_int_tensor**3-phi_int_tensor))**2)\n",
        "\n",
        "          bound_samp=100\n",
        "\n",
        "          xlu=np.random.uniform(0.00001,0.99999,(bound_samp,2))\n",
        "          xlu[:,0]=0.0\n",
        "          xlu_tensor=Variable(torch.from_numpy(xlu),requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "          xru=np.random.uniform(0.00001,0.99999,(bound_samp,2))\n",
        "          xru[:,0]=1.0\n",
        "          xru_tensor=Variable(torch.from_numpy(xru),requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "          xut=np.random.uniform(0.00001,0.99999,(bound_samp,2))\n",
        "          xut[:,1]=1.0\n",
        "          xut_tensor=Variable(torch.from_numpy(xut),requires_grad=True).to(device)\n",
        "\n",
        "          xub=np.random.uniform(0.00001,0.99999,(bound_samp,2))\n",
        "          xub[:,1]=0.0\n",
        "          xub_tensor=Variable(torch.from_numpy(xub),requires_grad=True).to(device)\n",
        "\n",
        "          phi_lu=model(xlu_tensor)\n",
        "          phi_ru=model(xru_tensor)\n",
        "          phi_ut=model(xut_tensor)\n",
        "          phi_ub=model(xub_tensor)\n",
        "\n",
        "          phi_p1_np=1.0*np.ones((bound_samp))\n",
        "          phi_m1_np=-1.0*np.ones((bound_samp))\n",
        "\n",
        "          phi_p1=Variable(torch.from_numpy(phi_p1_np),requires_grad=True).to(device)\n",
        "          phi_m1=Variable(torch.from_numpy(phi_m1_np),requires_grad=True).to(device)\n",
        "\n",
        "          loss_bound=torch.mean((phi_lu.reshape(-1)-phi_p1)**2)+torch.mean((phi_ru.reshape(-1)-phi_p1)**2)+torch.mean((phi_ut.reshape(-1)-phi_m1)**2)+torch.mean((phi_ub.reshape(-1)-phi_m1)**2)\n",
        "          loss = alpha1*loss_res +alpha5*loss_bound\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i) % 50 == 0:\n",
        "            x1 = np.linspace(0.001,0.999,dn,endpoint=True)\n",
        "            x2 = np.linspace(0.001,0.999,dn,endpoint=True)\n",
        "\n",
        "            xx1,xx2=np.meshgrid(x1,x2)\n",
        "            xx1=xx1.reshape((-1,1))\n",
        "            xx2=xx2.reshape((-1,1))\n",
        "\n",
        "            xx=np.hstack((xx1,xx2))\n",
        "\n",
        "            xx_tensor=Variable(torch.from_numpy(xx),requires_grad=True).to(device)\n",
        "\n",
        "            xx_all=[]\n",
        "            for j in range(x_dim):\n",
        "              xx_all.append(xx_tensor[:,j:j+1])\n",
        "\n",
        "            xx_int_tensor=torch.cat(xx_all,1)\n",
        "\n",
        "            phi=model(xx_int_tensor)\n",
        "\n",
        "            gradxx_all=[]\n",
        "            abs_gradxx_phi_2=0.0\n",
        "            for j in range(x_dim):\n",
        "              gradxx_int = torch.autograd.grad(outputs=phi,inputs=xx_all[j],grad_outputs=torch.ones_like(xx_all[j]),create_graph=True)[0]\n",
        "              gradxx_all.append(gradxx_int)\n",
        "\n",
        "            gradxx2_int=0.0\n",
        "            for j in range(x_dim):\n",
        "              gradxx2_j=torch.autograd.grad(outputs=gradxx_all[j],inputs=xx_all[j],grad_outputs=torch.ones_like(xx_all[j]),create_graph=True)[0]\n",
        "              gradxx2_int=gradxx2_int+gradxx2_j\n",
        "\n",
        "\n",
        "            L_phi=(kappa**2*gradxx2_int-(phi**3-phi))**2\n",
        "            L_int=torch.mean(L_phi,0)\n",
        "\n",
        "            lg_batch.append(L_int.item())\n",
        "\n",
        "            if i<pre_train_step:\n",
        "              loss,batch=loss.item(),i\n",
        "              print(\"iteration=\",i,\"loss=\", loss)\n",
        "            else:\n",
        "              loss, batch = alpha1*loss_res.item()+alpha5*loss_bound.item(), i\n",
        "              print(f'batches: {batch+1}')\n",
        "              print(f'loss1: {loss_res} loss_bound: {loss_bound}')\n",
        "\n",
        "            loss_batch.append(loss)\n",
        "            plt_batch.append(i+1)\n",
        "            plt.figure()\n",
        "\n",
        "        if (i) % 500 == 0:\n",
        "            fig_cos_pre(plt_batch,lg_batch,i)\n",
        "            fig_loss_batch_pre(plt_batch,loss_batch)\n",
        "            fig_countour_pre(xx,phi.cpu().detach().numpy(),i)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            torch.save(model.state_dict(), model_name)\n",
        "            print(\"Saved PyTorch Model State to \" +str(model_name))\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "    mkdir_pre(all_path+casenum+\"/\")\n",
        "    print(all_path+casenum+\"/\")\n",
        "\n",
        "\n",
        "    model = NeuralNetwork().to(device)\n",
        "    model = model.double()\n",
        "\n",
        "    #model = train_pre(model)\n",
        "    train(model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi_kS3JLN5xE"
      },
      "source": [
        "# Generate minimals (1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOGEugCnN5xP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pickle\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device to train\")\n",
        "\n",
        "mycase=\"find_minimals\"\n",
        "casenum=\"p3_k005\"\n",
        "\n",
        "\n",
        "all_path=\"/content/drive/MyDrive/Colab_Notebooks/2023_mep/0120/\"\n",
        "\n",
        "model_name=all_path+casenum+\"/\"+\"model_MEP_\"+mycase+\"_\"+casenum+\".pth\" # Path\n",
        "\n",
        "\n",
        "alpha1=10000\n",
        "alpha5=1\n",
        "kappa=0.05\n",
        "\n",
        "\n",
        "load_model = False\n",
        "batches = 10000\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "dn=31\n",
        "x_dim=2\n",
        "\n",
        "pre_train_step=500\n",
        "\n",
        "\n",
        "def mkdir_pre(path):\n",
        "  folder = os.path.exists(path)\n",
        "  if not folder:\n",
        "    os.makedirs(path)\n",
        "    print(\"---  new folder...  ---\")\n",
        "    print(\"---  OK  ---\")\n",
        "  else:\n",
        "    print(\"---  There is this folder!  ---\")\n",
        "\n",
        "\n",
        "def fig_loss_batch_pre(plt_batch,loss_batch):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.semilogy(plt_batch,loss_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "def fig_cos_pre(plt_batch,lg_batch,j):\n",
        "    fig = plt.figure(figsize=(5,4))\n",
        "    plt.semilogy(plt_batch,lg_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"loss on grid\")\n",
        "    if j == batches-500:\n",
        "      plt.savefig(all_path+casenum+\"/\"+\"lgloss\"+\".jpg\")\n",
        "\n",
        "\n",
        "def fig_countour_pre(xx,phi,j):\n",
        "    xx1=xx[:,0]\n",
        "    xx2=xx[:,1]\n",
        "\n",
        "    xx1=xx1.reshape(dn,dn)\n",
        "    xx2=xx2.reshape(dn,dn)\n",
        "\n",
        "    phi_plot_sq=phi.reshape(dn,dn)\n",
        "\n",
        "\n",
        "    fig2 = plt.figure(figsize=(24,4))\n",
        "    fig2.add_subplot(1,1,1)\n",
        "    picori=phi_plot_sq.reshape(dn,dn)\n",
        "    pic=np.zeros((dn,dn,3))\n",
        "\n",
        "    B=(np.sign(picori)+1)/2*picori\n",
        "    C=(np.sign(picori)-1)/2*picori\n",
        "\n",
        "\n",
        "    pic[:,:,0]=B\n",
        "    pic[:,:,-1]=C\n",
        "\n",
        "    plt.imshow(pic)\n",
        "\n",
        "\n",
        "    plt.savefig(all_path+casenum+\"/\"+str(j)+\".jpg\")\n",
        "\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_tanh_stack = nn.Sequential(\n",
        "            nn.Linear(2,32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, s):\n",
        "        phi_pred = self.linear_tanh_stack(s)\n",
        "        return phi_pred\n",
        "\n",
        "\n",
        "\n",
        "def train(model):\n",
        "    loss_batch=[]\n",
        "    plt_batch=[]\n",
        "    lg_batch=[]\n",
        "\n",
        "    for i in range(batches):\n",
        "        if i<2000:\n",
        "            learning_rate=1e-3\n",
        "        elif i<30000:\n",
        "            learning_rate=1e-4\n",
        "        else:\n",
        "            learning_rate=1e-5\n",
        "\n",
        "        if i<2000:\n",
        "          alpha1=10\n",
        "          alpha5=1\n",
        "        else:\n",
        "          alpha1=10\n",
        "          alpha5=1\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        datanum=500\n",
        "        x_np = np.random.uniform(0.001,0.999,(datanum,2))\n",
        "\n",
        "\n",
        "        x_tensor=Variable(torch.from_numpy(x_np),requires_grad=True).to(device)\n",
        "\n",
        "        x_all=[]\n",
        "        for j in range(x_dim):\n",
        "          x_all.append(x_tensor[:,j:j+1])\n",
        "\n",
        "        x_tensor_all=torch.cat(x_all,1)\n",
        "        phi_int_tensor=model(x_tensor_all)\n",
        "\n",
        "        if i<pre_train_step:\n",
        "          loss_res=torch.mean((phi_int_tensor-1)**2)\n",
        "          bound_samp=100\n",
        "\n",
        "          xlu=np.random.uniform(0.001,0.999,(bound_samp,2))\n",
        "          xlu[:,0]=0.0\n",
        "          xlu_tensor=Variable(torch.from_numpy(xlu),requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "          xru=np.random.uniform(0.001,0.999,(bound_samp,2))\n",
        "          xru[:,0]=1.0\n",
        "          xru_tensor=Variable(torch.from_numpy(xru),requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "          xut=np.random.uniform(0.001,0.999,(bound_samp,2))\n",
        "          xut[:,1]=1.0\n",
        "          xut_tensor=Variable(torch.from_numpy(xut),requires_grad=True).to(device)\n",
        "\n",
        "          xub=np.random.uniform(0.001,0.999,(bound_samp,2))\n",
        "          xub[:,1]=0.0\n",
        "          xub_tensor=Variable(torch.from_numpy(xub),requires_grad=True).to(device)\n",
        "\n",
        "          phi_lu=model(xlu_tensor)\n",
        "          phi_ru=model(xru_tensor)\n",
        "          phi_ut=model(xut_tensor)\n",
        "          phi_ub=model(xub_tensor)\n",
        "\n",
        "          phi_p1_np=1.0*np.ones((bound_samp))\n",
        "          phi_m1_np=-1.0*np.ones((bound_samp))\n",
        "\n",
        "          phi_p1=Variable(torch.from_numpy(phi_p1_np),requires_grad=True).to(device)\n",
        "          phi_m1=Variable(torch.from_numpy(phi_m1_np),requires_grad=True).to(device)\n",
        "\n",
        "          loss_bound=torch.mean((phi_lu.reshape(-1)-phi_p1)**2)+torch.mean((phi_ru.reshape(-1)-phi_p1)**2)+torch.mean((phi_ut.reshape(-1)-phi_m1)**2)+torch.mean((phi_ub.reshape(-1)-phi_m1)**2)\n",
        "\n",
        "          loss=alpha1*loss_res+alpha5*loss_bound\n",
        "\n",
        "        else:\n",
        "          gradx_all=[]\n",
        "          for j in range(x_dim):\n",
        "            gradx_int = torch.autograd.grad(outputs=phi_int_tensor,inputs=x_all[j],grad_outputs=torch.ones_like(x_tensor[:,j:j+1]),create_graph=True)[0]\n",
        "            gradx_all.append(gradx_int)\n",
        "\n",
        "          gradx2_int=0.0\n",
        "          for j in range(x_dim):\n",
        "            gradx2_j=torch.autograd.grad(outputs=gradx_all[j],inputs=x_all[j],grad_outputs=torch.ones_like(x_tensor[:,j:j+1]),create_graph=True)[0]\n",
        "            gradx2_int=gradx2_int+gradx2_j\n",
        "\n",
        "\n",
        "          loss_res=torch.mean((kappa**2*gradx2_int-(phi_int_tensor**3-phi_int_tensor))**2)\n",
        "\n",
        "          bound_samp=100\n",
        "\n",
        "          xlu=np.random.uniform(0.00001,0.99999,(bound_samp,2))\n",
        "          xlu[:,0]=0.0\n",
        "          xlu_tensor=Variable(torch.from_numpy(xlu),requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "          xru=np.random.uniform(0.00001,0.99999,(bound_samp,2))\n",
        "          xru[:,0]=1.0\n",
        "          xru_tensor=Variable(torch.from_numpy(xru),requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "          xut=np.random.uniform(0.00001,0.99999,(bound_samp,2))\n",
        "          xut[:,1]=1.0\n",
        "          xut_tensor=Variable(torch.from_numpy(xut),requires_grad=True).to(device)\n",
        "\n",
        "          xub=np.random.uniform(0.00001,0.99999,(bound_samp,2))\n",
        "          xub[:,1]=0.0\n",
        "          xub_tensor=Variable(torch.from_numpy(xub),requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "          phi_lu=model(xlu_tensor)\n",
        "          phi_ru=model(xru_tensor)\n",
        "          phi_ut=model(xut_tensor)\n",
        "          phi_ub=model(xub_tensor)\n",
        "\n",
        "          phi_p1_np=1.0*np.ones((bound_samp))\n",
        "          phi_m1_np=-1.0*np.ones((bound_samp))\n",
        "\n",
        "          phi_p1=Variable(torch.from_numpy(phi_p1_np),requires_grad=True).to(device)\n",
        "          phi_m1=Variable(torch.from_numpy(phi_m1_np),requires_grad=True).to(device)\n",
        "\n",
        "          loss_bound=torch.mean((phi_lu.reshape(-1)-phi_p1)**2)+torch.mean((phi_ru.reshape(-1)-phi_p1)**2)+torch.mean((phi_ut.reshape(-1)-phi_m1)**2)+torch.mean((phi_ub.reshape(-1)-phi_m1)**2)\n",
        "          loss = alpha1*loss_res +alpha5*loss_bound\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i) % 50 == 0:\n",
        "            x1 = np.linspace(0.001,0.999,dn,endpoint=True)\n",
        "            x2 = np.linspace(0.001,0.999,dn,endpoint=True)\n",
        "\n",
        "            xx1,xx2=np.meshgrid(x1,x2)\n",
        "            xx1=xx1.reshape((-1,1))\n",
        "            xx2=xx2.reshape((-1,1))\n",
        "\n",
        "            xx=np.hstack((xx1,xx2))\n",
        "\n",
        "            xx_tensor=Variable(torch.from_numpy(xx),requires_grad=True).to(device)\n",
        "\n",
        "            xx_all=[]\n",
        "            for j in range(x_dim):\n",
        "              xx_all.append(xx_tensor[:,j:j+1])\n",
        "\n",
        "            xx_int_tensor=torch.cat(xx_all,1)\n",
        "\n",
        "            phi=model(xx_int_tensor)\n",
        "\n",
        "            gradxx_all=[]\n",
        "            abs_gradxx_phi_2=0.0\n",
        "            for j in range(x_dim):\n",
        "              gradxx_int = torch.autograd.grad(outputs=phi,inputs=xx_all[j],grad_outputs=torch.ones_like(xx_all[j]),create_graph=True)[0]\n",
        "              gradxx_all.append(gradxx_int)\n",
        "\n",
        "\n",
        "            gradxx2_int=0.0\n",
        "            for j in range(x_dim):\n",
        "              gradxx2_j=torch.autograd.grad(outputs=gradxx_all[j],inputs=xx_all[j],grad_outputs=torch.ones_like(xx_all[j]),create_graph=True)[0]\n",
        "              gradxx2_int=gradxx2_int+gradxx2_j\n",
        "\n",
        "\n",
        "            L_phi=(kappa**2*gradxx2_int-(phi**3-phi))**2\n",
        "            L_int=torch.mean(L_phi,0)\n",
        "\n",
        "            lg_batch.append(L_int.item())\n",
        "\n",
        "            if i<pre_train_step:\n",
        "              loss,batch=loss.item(),i\n",
        "              print(\"iteration=\",i,\"loss=\", loss)\n",
        "            else:\n",
        "              loss, batch = alpha1*loss_res.item()+alpha5*loss_bound.item(), i\n",
        "              print(f'batches: {batch+1}')\n",
        "              print(f'loss1: {loss_res} loss_bound: {loss_bound}')\n",
        "\n",
        "            loss_batch.append(loss)\n",
        "            plt_batch.append(i+1)\n",
        "            plt.figure()\n",
        "\n",
        "        if (i) % 500 == 0:\n",
        "            fig_cos_pre(plt_batch,lg_batch,i)\n",
        "            fig_loss_batch_pre(plt_batch,loss_batch)\n",
        "            fig_countour_pre(xx,phi.cpu().detach().numpy(),i)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            torch.save(model.state_dict(), model_name)\n",
        "            print(\"Saved PyTorch Model State to \" +str(model_name))\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "    mkdir_pre(all_path+casenum+\"/\")\n",
        "    print(all_path+casenum+\"/\")\n",
        "\n",
        "\n",
        "    model = NeuralNetwork().to(device)\n",
        "    model = model.double()\n",
        "\n",
        "    #model = train_pre(model)\n",
        "    train(model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X-2qpDvCS7-"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6hHfmpy8UQ8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pickle\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device to train\")\n",
        "\n",
        "\n",
        "all_path=\"/content/drive/MyDrive/Colab_Notebooks/2023_mep/0120/\"\n",
        "path_m1=\"/content/drive/MyDrive/Colab_Notebooks/2023_mep/0120/m3_k005/model_MEP_find_minimals_m3_k005.pth\"\n",
        "path_p1=\"/content/drive/MyDrive/Colab_Notebooks/2023_mep/0120/p3_k005/model_MEP_find_minimals_p3_k005.pth\"\n",
        "\n",
        "\n",
        "mycase=\"ac_2d\"\n",
        "casenum=\"NBC3_k005_init3_beta10lg_13\"\n",
        "\n",
        "path_lg=all_path+casenum+\"/\"+\"lgloss\"+\".pkl\"\n",
        "\n",
        "plot_use_all=[]\n",
        "\n",
        "torch.set_printoptions(precision=10)\n",
        "\n",
        "\n",
        "model_name= all_path+casenum+\"/\"+\"model_MEP_\"+mycase+\"_\"+casenum+\".pth\"\n",
        "model_Parameters_name=model_name\n",
        "\n",
        "\n",
        "kappa=0.05\n",
        "load_model = False\n",
        "batches = 20000\n",
        "beta = 10\n",
        "learning_rate = 1e-4\n",
        "dimension = 1\n",
        "dn=28\n",
        "x_dim=2\n",
        "\n",
        "\n",
        "\n",
        "alpha1=1\n",
        "alpha4=0\n",
        "alpha3=0.001\n",
        "alpha5=1\n",
        "\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "  folder = os.path.exists(path)\n",
        "  if not folder:\n",
        "    os.makedirs(path)\n",
        "    print(\"---  new folder...  ---\")\n",
        "    print(\"---  OK  ---\")\n",
        "  else:\n",
        "    print(\"---  There is this folder!  ---\")\n",
        "\n",
        "\n",
        "def fig_loss_batch(plt_batch,loss_batch):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.plot(plt_batch,loss_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "def fig_cos(plt_batch,lg_batch,j):\n",
        "    fig = plt.figure(figsize=(5,4))\n",
        "    plt.plot(plt_batch,lg_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"$l_g$\")\n",
        "    if j == batches-500:\n",
        "      plt.savefig(all_path+casenum+\"/\"+\"lgloss\"+\".jpg\")\n",
        "\n",
        "\n",
        "\n",
        "def fig_countour(xx,phi,ep,j):\n",
        "    xx1=xx[:,0]\n",
        "    xx2=xx[:,1]\n",
        "\n",
        "    xx1=xx1.reshape(dn**2,dn)\n",
        "    xx2=xx2.reshape(dn**2,dn)\n",
        "\n",
        "    phi_plot_sq=phi.reshape(dn**2,dn)\n",
        "\n",
        "\n",
        "    fig1 = plt.figure(figsize=(24,8))\n",
        "    dn_draw=list(range(0,dn,int(dn/9)))\n",
        "\n",
        "    count=1\n",
        "\n",
        "\n",
        "    for i in dn_draw:\n",
        "      fig1.add_subplot(2,5,count)\n",
        "      plt.contourf(xx1[:,i].reshape(dn,dn),xx2[:,i].reshape(dn,dn),phi_plot_sq[:,i].reshape(dn,dn))\n",
        "      plt.title(\"Energy:\"+str(ep[i]))\n",
        "      count=count+1\n",
        "\n",
        "    fig2 = plt.figure(figsize=(24,8))\n",
        "    count=1\n",
        "\n",
        "    for u in dn_draw:\n",
        "      fig2.add_subplot(2,5,count)\n",
        "      picori=phi_plot_sq[:,u].reshape(dn,dn)\n",
        "      pic=np.zeros((dn,dn,3))\n",
        "\n",
        "      B=(np.sign(picori)+1)/2*picori\n",
        "      C=(np.sign(picori)-1)/2*picori\n",
        "\n",
        "      pic[:,:,0]=B\n",
        "      pic[:,:,-1]=C\n",
        "\n",
        "      ee=round(ep[u],3)\n",
        "      plt.imshow(pic)\n",
        "      plt.title(\"Energy:\"+str(ee))\n",
        "      count=count+1\n",
        "\n",
        "\n",
        "    plt.savefig(all_path+casenum+\"/\"+str(j)+\".pdf\")\n",
        "\n",
        "    fig3 = plt.figure(figsize=(24,8))\n",
        "    count=1\n",
        "\n",
        "    for u in dn_draw:\n",
        "      fig3.add_subplot(2,5,count)\n",
        "      picori=phi_plot_sq[:,u].reshape(dn,dn)\n",
        "      pic=np.zeros((dn,dn,3))\n",
        "\n",
        "      B=(np.sign(picori)+1)/2*picori\n",
        "      C=(np.sign(picori)-1)/2*picori\n",
        "\n",
        "      pic[:,:,0]=B\n",
        "      pic[:,:,-1]=C\n",
        "\n",
        "      ee=round(ep[u],3)\n",
        "      plt.imshow(pic)\n",
        "      count=count+1\n",
        "\n",
        "\n",
        "    plt.savefig(all_path+casenum+\"/\"+str(j)+\"notitle.pdf\")\n",
        "\n",
        "\n",
        "class NeuralNetwork_minimum(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork_minimum, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_tanh_stack = nn.Sequential(\n",
        "            nn.Linear(2,32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, s):\n",
        "        phi_pred = self.linear_tanh_stack(s)\n",
        "        return phi_pred\n",
        "\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self,p1,m1):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_tanh_stack = nn.Sequential(\n",
        "            nn.Linear(3,100),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(100, 1)\n",
        "        )\n",
        "        self.p1=p1\n",
        "        self.m1=m1\n",
        "\n",
        "\n",
        "    def forward(self, s):\n",
        "        x_pred = self.linear_tanh_stack(s)\n",
        "        ss=s[:,0]\n",
        "        ss=ss.reshape(-1,1)\n",
        "        xx=s[:,1:3]\n",
        "\n",
        "        out=ss*(1-ss)*x_pred + (1-ss)*self.p1(xx) + ss*self.m1(xx)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def train(model):\n",
        "    loss_batch=[]\n",
        "    plt_batch=[]\n",
        "    lg_batch=[]\n",
        "\n",
        "    if load_model:\n",
        "        model.load_state_dict(torch.load(model_Parameters_name))\n",
        "\n",
        "    for i in range(batches):\n",
        "        if i<5000:\n",
        "            learning_rate=1e-3\n",
        "        else:\n",
        "            learning_rate=1e-4\n",
        "\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        if i<2000:\n",
        "          alpha1=10\n",
        "          alpha4=1\n",
        "          alpha3=0.001\n",
        "          alpha5=1\n",
        "\n",
        "        else:\n",
        "          alpha1=0\n",
        "          alpha4=1\n",
        "          alpha3=0.001\n",
        "          alpha5=0.01\n",
        "\n",
        "\n",
        "        datanum=50\n",
        "        s_np = np.random.uniform(0.001,0.999,(datanum,1))\n",
        "        x_np = np.random.uniform(0.001,0.999,(datanum,2))\n",
        "\n",
        "        xxind=np.linspace(0,datanum-1,datanum)\n",
        "        s_int_mesh_np,xxind_mesh_np=np.meshgrid(s_np,xxind)\n",
        "\n",
        "        s_int_np=s_int_mesh_np.reshape((-1,1))\n",
        "        xxind_np=xxind_mesh_np.reshape(-1)\n",
        "\n",
        "        x_int_np=x_np[xxind_np.astype(np.int32),:]\n",
        "\n",
        "        s_int_tensor=Variable(torch.from_numpy(s_int_np),requires_grad=True).to(device)\n",
        "        x_int_tensor1=Variable(torch.from_numpy(x_int_np),requires_grad=True).to(device)\n",
        "\n",
        "        x_all=[]\n",
        "        for j in range(x_dim):\n",
        "          x_all.append(x_int_tensor1[:,j:j+1])\n",
        "\n",
        "        x_int_tensor=torch.cat(x_all,1)\n",
        "\n",
        "        sx=torch.cat((s_int_tensor,x_int_tensor),1)\n",
        "\n",
        "        phi_int_tensor=model(sx)\n",
        "\n",
        "\n",
        "        gradx_all=[]\n",
        "        abs_gradx_phi_2=0.0\n",
        "        for j in range(x_dim):\n",
        "          gradx_int = torch.autograd.grad(outputs=phi_int_tensor,inputs=x_all[j],grad_outputs=torch.ones_like(s_int_tensor),create_graph=True)[0]\n",
        "          gradx_all.append(gradx_int)\n",
        "          abs_gradx_phi_2=abs_gradx_phi_2+gradx_int**2\n",
        "\n",
        "        V_int = kappa**2/2*abs_gradx_phi_2+(phi_int_tensor**2-1)**2/4\n",
        "\n",
        "        V_int_mesh=V_int.reshape((datanum,datanum))\n",
        "        F_aint=torch.mean(V_int_mesh,0)\n",
        "\n",
        "        loss_1=1/beta*torch.log(torch.mean(torch.exp(beta*F_aint)))\n",
        "\n",
        "        grads_int = torch.autograd.grad(outputs=phi_int_tensor,inputs=s_int_tensor,grad_outputs=torch.ones_like(s_int_tensor),create_graph=True)[0]\n",
        "        grads2_int = torch.autograd.grad(outputs=grads_int,inputs=s_int_tensor,grad_outputs=torch.ones_like(s_int_tensor),create_graph=True)[0]\n",
        "        loss_3=torch.mean(torch.abs(grads_int*grads2_int))\n",
        "\n",
        "        gradx2_int=0.0\n",
        "        for j in range(x_dim):\n",
        "          gradx2_j=torch.autograd.grad(outputs=gradx_all[j],inputs=x_all[j],grad_outputs=torch.ones_like(s_int_tensor),create_graph=True)[0]\n",
        "          gradx2_int=gradx2_int+gradx2_j\n",
        "\n",
        "\n",
        "        L_phi_mesh_pre=(kappa**2*gradx2_int-(phi_int_tensor**3-phi_int_tensor))**2\n",
        "        L_phi_mesh=L_phi_mesh_pre.reshape((datanum,datanum))\n",
        "        L_int=torch.mean(L_phi_mesh,0)\n",
        "\n",
        "        phi_s_mesh=grads_int.reshape((datanum,datanum))\n",
        "        phi_int=torch.mean(phi_s_mesh**2,0)\n",
        "\n",
        "        loss_4=torch.mean(torch.sqrt(L_int)*torch.sqrt(phi_int))\n",
        "\n",
        "        s_b = np.random.uniform(0.001,0.999,(100,1))\n",
        "        s_b_tensor=Variable(torch.from_numpy(s_b),requires_grad=True).to(device)\n",
        "\n",
        "        xlu=np.random.uniform(0.001,0.999,(100,2))\n",
        "        xlu[:,0]=0.0\n",
        "        xlu_tensor=Variable(torch.from_numpy(xlu),requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "        xru=np.random.uniform(0.001,0.999,(100,2))\n",
        "        xru[:,0]=1.0\n",
        "        xru_tensor=Variable(torch.from_numpy(xru),requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "        xut=np.random.uniform(0.001,0.999,(100,2))\n",
        "        xut[:,1]=1.0\n",
        "        xut_tensor=Variable(torch.from_numpy(xut),requires_grad=True).to(device)\n",
        "\n",
        "        xub=np.random.uniform(0.001,0.999,(100,2))\n",
        "        xub[:,1]=0.0\n",
        "        xub_tensor=Variable(torch.from_numpy(xub),requires_grad=True).to(device)\n",
        "\n",
        "\n",
        "        sx_lu=torch.cat((s_b_tensor,xlu_tensor),1)\n",
        "        sx_ru=torch.cat((s_b_tensor,xru_tensor),1)\n",
        "        sx_ut=torch.cat((s_b_tensor,xut_tensor),1)\n",
        "        sx_ub=torch.cat((s_b_tensor,xub_tensor),1)\n",
        "\n",
        "\n",
        "        phi_lu=model(sx_lu)\n",
        "        phi_ru=model(sx_ru)\n",
        "        phi_ut=model(sx_ut)\n",
        "        phi_ub=model(sx_ub)\n",
        "\n",
        "        phi_p1_np=1.0*np.ones((100))\n",
        "        phi_m1_np=-1.0*np.ones((100))\n",
        "\n",
        "        phi_p1=Variable(torch.from_numpy(phi_p1_np),requires_grad=True).to(device)\n",
        "        phi_m1=Variable(torch.from_numpy(phi_m1_np),requires_grad=True).to(device)\n",
        "\n",
        "        loss_bound=torch.mean((phi_lu.reshape(-1)-phi_p1)**2)+torch.mean((phi_ru.reshape(-1)-phi_p1)**2)+torch.mean((phi_ut.reshape(-1)-phi_m1)**2)+torch.mean((phi_ub.reshape(-1)-phi_m1)**2)\n",
        "\n",
        "\n",
        "\n",
        "        loss = alpha1*loss_1 +alpha4*loss_4+alpha3*loss_3+alpha5*loss_bound\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i) % 50 == 0:\n",
        "            ss = np.linspace(0.001,0.999,dn,endpoint=True)\n",
        "            x1 = np.linspace(0.001,0.999,dn,endpoint=True)\n",
        "            x2 = np.linspace(0.001,0.999,dn,endpoint=True)\n",
        "\n",
        "            xx1,xx2=np.meshgrid(x1,x2)\n",
        "            xx1=xx1.reshape((-1,1))\n",
        "            xx2=xx2.reshape((-1,1))\n",
        "\n",
        "            xx=np.hstack((xx1,xx2))\n",
        "\n",
        "            xxind=np.linspace(0,dn**2-1,dn**2)\n",
        "\n",
        "            ss_int_mesh_np,xxind_int_mesh_np=np.meshgrid(ss,xxind)\n",
        "\n",
        "            ss_int_np=ss_int_mesh_np.reshape((-1,1))\n",
        "            xxind_np=xxind_int_mesh_np.reshape(-1)\n",
        "\n",
        "            xx_int_np=xx[xxind_np.astype(np.int32),:]\n",
        "\n",
        "            ss_int_tensor=Variable(torch.from_numpy(ss_int_np),requires_grad=True).to(device)\n",
        "            xx_int_tensor1=Variable(torch.from_numpy(xx_int_np),requires_grad=True).to(device)\n",
        "\n",
        "            xx_all=[]\n",
        "            for j in range(x_dim):\n",
        "              xx_all.append(xx_int_tensor1[:,j:j+1])\n",
        "\n",
        "            xx_int_tensor=torch.cat(xx_all,1)\n",
        "\n",
        "            ssxx=torch.cat((ss_int_tensor,xx_int_tensor),1)\n",
        "\n",
        "            phi=model(ssxx)\n",
        "\n",
        "            gradxx_all=[]\n",
        "            abs_gradxx_phi_2=0.0\n",
        "            for j in range(x_dim):\n",
        "              gradxx_int = torch.autograd.grad(outputs=phi,inputs=xx_all[j],grad_outputs=torch.ones_like(ss_int_tensor),create_graph=True)[0]\n",
        "              gradxx_all.append(gradxx_int)\n",
        "              abs_gradxx_phi_2=abs_gradxx_phi_2+gradxx_int**2\n",
        "\n",
        "\n",
        "            gradxx2_int=0.0\n",
        "            for j in range(x_dim):\n",
        "              gradxx2_j=torch.autograd.grad(outputs=gradxx_all[j],inputs=xx_all[j],grad_outputs=torch.ones_like(ss_int_tensor),create_graph=True)[0]\n",
        "              gradxx2_int=gradxx2_int+gradxx2_j\n",
        "\n",
        "\n",
        "            L_phi_mesh_pre=(kappa**2*gradxx2_int-(phi**3-phi))**2\n",
        "            L_phi_mesh=L_phi_mesh_pre.reshape((dn**2,dn))\n",
        "            L_int=torch.mean(L_phi_mesh,0)\n",
        "\n",
        "            gradss_int = torch.autograd.grad(outputs=phi,inputs=ss_int_tensor,grad_outputs=torch.ones_like(ss_int_tensor),create_graph=True)[0]\n",
        "\n",
        "            phi_ss_mesh=gradss_int.reshape((dn**2,dn))\n",
        "\n",
        "            phi_int=torch.mean(phi_ss_mesh**2,0)\n",
        "\n",
        "            lg=torch.mean(torch.sqrt(L_int)*torch.sqrt(phi_int))\n",
        "\n",
        "            lg_batch.append(lg.item())\n",
        "\n",
        "            loss, batch = alpha1*loss_1.item() + alpha4*loss_4.item() + alpha3*loss_3.item()+alpha5*loss_bound.item(), i\n",
        "            print(f'batches: {batch+1}')\n",
        "            print(f'loss1: {alpha1*loss_1} loss4: {alpha4*loss_4} loss3: {alpha3*loss_3} loss_bound: {alpha5*loss_bound}')\n",
        "\n",
        "            loss_batch.append(loss)\n",
        "            plt_batch.append(i+1)\n",
        "            plt.figure()\n",
        "\n",
        "        if (i) % 500 == 0:\n",
        "\n",
        "            V_int = kappa**2/2*abs_gradxx_phi_2+(phi**2-1)**2/4\n",
        "\n",
        "            V_int_mesh=V_int.reshape((dn**2,dn))\n",
        "            F_aint=torch.mean(V_int_mesh,0)\n",
        "            energy_plot=F_aint.cpu().detach().numpy()\n",
        "\n",
        "            fig_cos(plt_batch,lg_batch,i)\n",
        "            fig_loss_batch(plt_batch,loss_batch)\n",
        "            fig_countour(xx_int_np,phi.cpu().detach().numpy(),energy_plot,i)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            torch.save(model.state_dict(), model_name)\n",
        "            print(\"Saved PyTorch Model State to \" +str(model_name))\n",
        "\n",
        "    output = open(path_lg, 'wb')\n",
        "    pickle.dump(lg_batch,output)\n",
        "    output.close()\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "    net_p1=NeuralNetwork_minimum().to(device)\n",
        "    net_p1=net_p1.double()\n",
        "    net_p1.load_state_dict(torch.load(path_p1))\n",
        "\n",
        "    for p in net_p1.parameters():\n",
        "        p.requires_grad=False\n",
        "\n",
        "\n",
        "    net_m1=NeuralNetwork_minimum().to(device)\n",
        "    net_m1=net_m1.double()\n",
        "    net_m1.load_state_dict(torch.load(path_m1))\n",
        "\n",
        "\n",
        "    for p in net_m1.parameters():\n",
        "        p.requires_grad=False\n",
        "\n",
        "\n",
        "    mkdir(all_path+casenum+\"/\")\n",
        "\n",
        "\n",
        "    model = NeuralNetwork(net_p1,net_m1).to(device)\n",
        "    model = model.double()\n",
        "\n",
        "    #model = train_pre(model)\n",
        "    train(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxXoPbavmPCp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4M7kUw2a_bKa"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}