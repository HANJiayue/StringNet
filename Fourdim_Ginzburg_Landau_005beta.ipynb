{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generate minimals (-1)"
      ],
      "metadata": {
        "id": "JHvjazHR-j1b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uQdtK3_-Y9A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pickle\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device to train\")\n",
        "\n",
        "mycase=\"find_minimals\"\n",
        "casenum=\"d4_m2_k005\"\n",
        "\n",
        "\n",
        "all_path=\"/content/drive/MyDrive/Colab_Notebooks/2023_MEP/0408/\"\n",
        "model_Parameters_name = all_path+casenum+\"/\"+\"model_MEP_\"+mycase[0:2]+\"_\"+\"pre2\"+\"pretrain\"+\".pth\" # Path\n",
        "\n",
        "model_name=all_path+casenum+\"/\"+\"model_MEP_\"+mycase+\"_\"+casenum+\".pth\" # Path\n",
        "\n",
        "\n",
        "alpha1=10000\n",
        "alpha5=1\n",
        "kappa=0.05\n",
        "\n",
        "\n",
        "load_model = False\n",
        "batches = 20000\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "dn=31\n",
        "x_dim=4\n",
        "\n",
        "pre_train_step=8000\n",
        "\n",
        "\n",
        "def mkdir_pre(path):\n",
        "  folder = os.path.exists(path)\n",
        "  if not folder:\n",
        "    os.makedirs(path)\n",
        "    print(\"---  new folder...  ---\")\n",
        "    print(\"---  OK  ---\")\n",
        "  else:\n",
        "    print(\"---  There is this folder!  ---\")\n",
        "\n",
        "\n",
        "def fig_loss_batch_pre(plt_batch,loss_batch):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.semilogy(plt_batch,loss_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "def fig_cos_pre(plt_batch,lg_batch,j):\n",
        "    fig = plt.figure(figsize=(5,4))\n",
        "    plt.semilogy(plt_batch,lg_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"loss on grid\")\n",
        "    if j == batches-500:\n",
        "      plt.savefig(all_path+casenum+\"/\"+\"lgloss\"+\".jpg\")\n",
        "\n",
        "\n",
        "def fig_countour_pre(model,j):\n",
        "    plot_use=[0,0.25,0.5,0.75,1]\n",
        "\n",
        "    x1 = np.linspace(0.001,0.999,dn,endpoint=True)\n",
        "    x2 = np.linspace(0.001,0.999,dn,endpoint=True)\n",
        "\n",
        "    xx1,xx2=np.meshgrid(x1,x2)\n",
        "    xx1=xx1.reshape((-1,1))\n",
        "    xx2=xx2.reshape((-1,1))\n",
        "\n",
        "    xx=np.hstack((xx1,xx2))\n",
        "\n",
        "    xx1=xx[:,0]\n",
        "    xx2=xx[:,1]\n",
        "\n",
        "    xx1=xx1.reshape(dn,dn)\n",
        "    xx2=xx2.reshape(dn,dn)\n",
        "\n",
        "    fig2 = plt.figure(figsize=(16,16))\n",
        "\n",
        "    for k in range(len(plot_use)):\n",
        "      for l in range(len(plot_use)):\n",
        "        x2=plot_use[k]*np.ones((dn*dn,1))\n",
        "        x3=plot_use[l]*np.ones((dn*dn,1))\n",
        "        xxx=np.hstack((xx,x2))\n",
        "        xxxx=np.hstack((xxx,x3))\n",
        "        xxxx_tensor=Variable(torch.from_numpy(xxxx),requires_grad=True).to(device)\n",
        "        phi_tensor=model(xxxx_tensor)\n",
        "\n",
        "        phi=phi_tensor.cpu().detach().numpy()\n",
        "\n",
        "        phi_plot_sq=phi.reshape(dn,dn)\n",
        "\n",
        "\n",
        "        fig2.add_subplot(len(plot_use),len(plot_use),k*len(plot_use)+l+1)\n",
        "\n",
        "        picori=phi_plot_sq.reshape(dn,dn)\n",
        "        pic=np.zeros((dn,dn,3))\n",
        "\n",
        "        B=(np.sign(picori)+1)/2*picori\n",
        "        C=(np.sign(picori)-1)/2*picori\n",
        "\n",
        "        pic[:,:,0]=B\n",
        "        pic[:,:,-1]=C\n",
        "\n",
        "\n",
        "        plt.imshow(pic)\n",
        "\n",
        "\n",
        "    plt.savefig(all_path+casenum+\"/\"+str(j)+\".jpg\")\n",
        "\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_tanh_stack = nn.Sequential(\n",
        "            nn.Linear(4,64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, s):\n",
        "        phi_pred = self.linear_tanh_stack(s)\n",
        "        return phi_pred\n",
        "\n",
        "\n",
        "\n",
        "def train(model):\n",
        "    loss_batch=[]\n",
        "    plt_batch=[]\n",
        "    lg_batch=[]\n",
        "\n",
        "    for i in range(batches):\n",
        "        if i<2000:\n",
        "            learning_rate=1e-3\n",
        "        elif i<15000:\n",
        "            learning_rate=1e-4\n",
        "        else:\n",
        "            learning_rate=1e-5\n",
        "\n",
        "        if i<2000:\n",
        "          alpha1=10\n",
        "          alpha5=10\n",
        "        else:\n",
        "          alpha1=10\n",
        "          alpha5=1\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "        datanum=500\n",
        "        x_np = np.random.uniform(0.001,0.999,(datanum,4))\n",
        "\n",
        "\n",
        "        x_tensor=Variable(torch.from_numpy(x_np),requires_grad=True).to(device)\n",
        "\n",
        "        x_all=[]\n",
        "        for j in range(x_dim):\n",
        "          x_all.append(x_tensor[:,j:j+1])\n",
        "\n",
        "        x_tensor_all=torch.cat(x_all,1)\n",
        "        phi_int_tensor=model(x_tensor_all)\n",
        "\n",
        "        bound_samp=100\n",
        "        x00=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x00[:,0]=0.0\n",
        "        x00_tensor=Variable(torch.from_numpy(x00),requires_grad=True).to(device)\n",
        "\n",
        "        x01=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x01[:,0]=1.0\n",
        "        x01_tensor=Variable(torch.from_numpy(x01),requires_grad=True).to(device)\n",
        "\n",
        "        x10=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x10[:,1]=0.0\n",
        "        x10_tensor=Variable(torch.from_numpy(x10),requires_grad=True).to(device)\n",
        "\n",
        "        x11=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x11[:,1]=1.0\n",
        "        x11_tensor=Variable(torch.from_numpy(x11),requires_grad=True).to(device)\n",
        "\n",
        "        x20=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x20[:,2]=0.0\n",
        "        x20_tensor=Variable(torch.from_numpy(x20),requires_grad=True).to(device)\n",
        "\n",
        "        x21=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x21[:,2]=1.0\n",
        "        x21_tensor=Variable(torch.from_numpy(x21),requires_grad=True).to(device)\n",
        "\n",
        "        x30=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x30[:,3]=0.0\n",
        "        x30_tensor=Variable(torch.from_numpy(x30),requires_grad=True).to(device)\n",
        "\n",
        "        x31=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x31[:,3]=1.0\n",
        "        x31_tensor=Variable(torch.from_numpy(x31),requires_grad=True).to(device)\n",
        "\n",
        "        phi_00=model(x00_tensor)\n",
        "        phi_01=model(x01_tensor)\n",
        "\n",
        "        phi_10=model(x10_tensor)\n",
        "        phi_11=model(x11_tensor)\n",
        "\n",
        "        phi_20=model(x20_tensor)\n",
        "        phi_21=model(x21_tensor)\n",
        "\n",
        "        phi_30=model(x30_tensor)\n",
        "        phi_31=model(x31_tensor)\n",
        "\n",
        "        phi_p1_np=1.0*np.ones((bound_samp))\n",
        "        phi_m1_np=-1.0*np.ones((bound_samp))\n",
        "\n",
        "        phi_p1=Variable(torch.from_numpy(phi_p1_np),requires_grad=True).to(device)\n",
        "        phi_m1=Variable(torch.from_numpy(phi_m1_np),requires_grad=True).to(device)\n",
        "\n",
        "        loss_bound=torch.mean((phi_00.reshape(-1)-phi_p1)**2)+torch.mean((phi_01.reshape(-1)-phi_p1)**2)+\\\n",
        "        torch.mean((phi_10.reshape(-1)-phi_m1)**2)+torch.mean((phi_11.reshape(-1)-phi_m1)**2)+\\\n",
        "        torch.mean((phi_20.reshape(-1)-phi_p1)**2)+torch.mean((phi_21.reshape(-1)-phi_p1)**2)+\\\n",
        "        torch.mean((phi_30.reshape(-1)-phi_m1)**2)+torch.mean((phi_31.reshape(-1)-phi_m1)**2)\n",
        "\n",
        "        if i<pre_train_step:\n",
        "          loss_res=torch.mean((phi_int_tensor+1)**2)\n",
        "          loss=alpha1*loss_res+alpha5*loss_bound\n",
        "\n",
        "        else:\n",
        "          gradx_all=[]\n",
        "          for j in range(x_dim):\n",
        "            gradx_int = torch.autograd.grad(outputs=phi_int_tensor,inputs=x_all[j],grad_outputs=torch.ones_like(x_tensor[:,j:j+1]),create_graph=True)[0]\n",
        "            gradx_all.append(gradx_int)\n",
        "\n",
        "          gradx2_int=0.0\n",
        "          for j in range(x_dim):\n",
        "            gradx2_j=torch.autograd.grad(outputs=gradx_all[j],inputs=x_all[j],grad_outputs=torch.ones_like(x_tensor[:,j:j+1]),create_graph=True)[0]\n",
        "            gradx2_int=gradx2_int+gradx2_j\n",
        "\n",
        "          loss_res=torch.mean((kappa**2*gradx2_int-(phi_int_tensor**3-phi_int_tensor))**2)\n",
        "\n",
        "          loss = alpha1*loss_res +alpha5*loss_bound\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i) % 50 == 0:\n",
        "            test_sample=10000\n",
        "\n",
        "            xx=np.random.uniform(0.001,0.999,(test_sample,4))\n",
        "\n",
        "            xx_tensor=Variable(torch.from_numpy(xx),requires_grad=True).to(device)\n",
        "\n",
        "            xx_all=[]\n",
        "            for j in range(x_dim):\n",
        "              xx_all.append(xx_tensor[:,j:j+1])\n",
        "\n",
        "            xx_int_tensor=torch.cat(xx_all,1)\n",
        "\n",
        "            phi=model(xx_int_tensor)\n",
        "\n",
        "            gradxx_all=[]\n",
        "            abs_gradxx_phi_2=0.0\n",
        "            for j in range(x_dim):\n",
        "              gradxx_int = torch.autograd.grad(outputs=phi,inputs=xx_all[j],grad_outputs=torch.ones_like(xx_all[j]),create_graph=True)[0]\n",
        "              gradxx_all.append(gradxx_int)\n",
        "\n",
        "\n",
        "            gradxx2_int=0.0\n",
        "            for j in range(x_dim):\n",
        "              gradxx2_j=torch.autograd.grad(outputs=gradxx_all[j],inputs=xx_all[j],grad_outputs=torch.ones_like(xx_all[j]),create_graph=True)[0]\n",
        "              gradxx2_int=gradxx2_int+gradxx2_j\n",
        "\n",
        "\n",
        "            L_phi=(kappa**2*gradxx2_int-(phi**3-phi))**2\n",
        "            L_int=torch.mean(L_phi,0)\n",
        "\n",
        "            lg_batch.append(L_int.item())\n",
        "\n",
        "            if i<pre_train_step:\n",
        "              loss,batch=loss.item(),i\n",
        "              print(\"iteration=\",i,\"loss=\", loss)\n",
        "            else:\n",
        "              loss, batch = alpha1*loss_res.item()+alpha5*loss_bound.item(), i\n",
        "              print(f'batches: {batch+1}')\n",
        "              print(f'loss1: {loss_res} loss_bound: {loss_bound}')\n",
        "\n",
        "            loss_batch.append(loss)\n",
        "            plt_batch.append(i+1)\n",
        "            plt.figure()\n",
        "\n",
        "        if (i) % 500 == 0:\n",
        "\n",
        "            fig_cos_pre(plt_batch,lg_batch,i)\n",
        "            fig_loss_batch_pre(plt_batch,loss_batch)\n",
        "            fig_countour_pre(model,i)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            torch.save(model.state_dict(), model_name)\n",
        "            print(\"Saved PyTorch Model State to \" +str(model_name))\n",
        "\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "    mkdir_pre(all_path+casenum+\"/\")\n",
        "    print(all_path+casenum+\"/\")\n",
        "\n",
        "\n",
        "    model = NeuralNetwork().to(device)\n",
        "    model = model.double()\n",
        "\n",
        "    train(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2XSLXllVaLq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate minimals (1)"
      ],
      "metadata": {
        "id": "jQ7VpM9d-tck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pickle\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device to train\")\n",
        "\n",
        "mycase=\"find_minimals\"\n",
        "casenum=\"d4_p1_k005\"\n",
        "\n",
        "\n",
        "all_path=\"/content/drive/MyDrive/Colab_Notebooks/2023_MEP/0408\"\n",
        "model_Parameters_name = all_path+casenum+\"/\"+\"model_MEP_\"+mycase[0:2]+\"_\"+\"pre2\"+\"pretrain\"+\".pth\" # Path\n",
        "\n",
        "model_name=all_path+casenum+\"/\"+\"model_MEP_\"+mycase+\"_\"+casenum+\".pth\" # Path\n",
        "\n",
        "\n",
        "alpha1=10000\n",
        "alpha5=1\n",
        "kappa=0.05\n",
        "\n",
        "\n",
        "load_model = False\n",
        "batches = 20000\n",
        "\n",
        "learning_rate = 1e-4\n",
        "\n",
        "dn=31\n",
        "x_dim=4\n",
        "\n",
        "pre_train_step=2000\n",
        "\n",
        "\n",
        "def mkdir_pre(path):\n",
        "  folder = os.path.exists(path)\n",
        "  if not folder:\n",
        "    os.makedirs(path)\n",
        "    print(\"---  new folder...  ---\")\n",
        "    print(\"---  OK  ---\")\n",
        "  else:\n",
        "    print(\"---  There is this folder!  ---\")\n",
        "\n",
        "\n",
        "def fig_loss_batch_pre(plt_batch,loss_batch):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.semilogy(plt_batch,loss_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "def fig_cos_pre(plt_batch,lg_batch,j):\n",
        "    fig = plt.figure(figsize=(5,4))\n",
        "    plt.semilogy(plt_batch,lg_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"loss on grid\")\n",
        "    if j == batches-500:\n",
        "      plt.savefig(all_path+casenum+\"/\"+\"lgloss\"+\".jpg\")\n",
        "\n",
        "\n",
        "def fig_countour_pre(model,j):\n",
        "    plot_use=[0,0.25,0.5,0.75,1]\n",
        "\n",
        "    x1 = np.linspace(0.001,0.999,dn,endpoint=True)\n",
        "    x2 = np.linspace(0.001,0.999,dn,endpoint=True)\n",
        "\n",
        "    xx1,xx2=np.meshgrid(x1,x2)\n",
        "    xx1=xx1.reshape((-1,1))\n",
        "    xx2=xx2.reshape((-1,1))\n",
        "\n",
        "    xx=np.hstack((xx1,xx2))\n",
        "\n",
        "    xx1=xx[:,0]\n",
        "    xx2=xx[:,1]\n",
        "\n",
        "    xx1=xx1.reshape(dn,dn)\n",
        "    xx2=xx2.reshape(dn,dn)\n",
        "\n",
        "    fig2 = plt.figure(figsize=(16,16))\n",
        "\n",
        "    for k in range(len(plot_use)):\n",
        "      for l in range(len(plot_use)):\n",
        "        x2=plot_use[k]*np.ones((dn*dn,1))\n",
        "        x3=plot_use[l]*np.ones((dn*dn,1))\n",
        "        xxx=np.hstack((xx,x2))\n",
        "        xxxx=np.hstack((xxx,x3))\n",
        "        xxxx_tensor=Variable(torch.from_numpy(xxxx),requires_grad=True).to(device)\n",
        "        phi_tensor=model(xxxx_tensor)\n",
        "\n",
        "        phi=phi_tensor.cpu().detach().numpy()\n",
        "\n",
        "        phi_plot_sq=phi.reshape(dn,dn)\n",
        "\n",
        "\n",
        "        fig2.add_subplot(len(plot_use),len(plot_use),k*len(plot_use)+l+1)\n",
        "\n",
        "        picori=phi_plot_sq.reshape(dn,dn)\n",
        "        pic=np.zeros((dn,dn,3))\n",
        "\n",
        "        B=(np.sign(picori)+1)/2*picori\n",
        "        C=(np.sign(picori)-1)/2*picori\n",
        "\n",
        "        pic[:,:,0]=B\n",
        "        pic[:,:,-1]=C\n",
        "\n",
        "        plt.imshow(pic)\n",
        "\n",
        "\n",
        "    plt.savefig(all_path+casenum+\"/\"+str(j)+\".jpg\")\n",
        "\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_tanh_stack = nn.Sequential(\n",
        "            nn.Linear(4,64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, s):\n",
        "        phi_pred = self.linear_tanh_stack(s)\n",
        "        return phi_pred\n",
        "\n",
        "\n",
        "\n",
        "def train(model):\n",
        "    loss_batch=[]\n",
        "    plt_batch=[]\n",
        "    lg_batch=[]\n",
        "\n",
        "    for i in range(batches):\n",
        "\n",
        "        if i<2000:\n",
        "            learning_rate=1e-3\n",
        "        elif i<15000:\n",
        "            learning_rate=1e-4\n",
        "        else:\n",
        "            learning_rate=1e-5\n",
        "\n",
        "        if i<2000:\n",
        "          alpha1=10\n",
        "          alpha5=10\n",
        "        else:\n",
        "          alpha1=10\n",
        "          alpha5=1\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "        datanum=500\n",
        "        x_np = np.random.uniform(0.001,0.999,(datanum,4))\n",
        "\n",
        "\n",
        "        x_tensor=Variable(torch.from_numpy(x_np),requires_grad=True).to(device)\n",
        "\n",
        "        x_all=[]\n",
        "        for j in range(x_dim):\n",
        "          x_all.append(x_tensor[:,j:j+1])\n",
        "\n",
        "        x_tensor_all=torch.cat(x_all,1)\n",
        "        phi_int_tensor=model(x_tensor_all)\n",
        "\n",
        "        bound_samp=100\n",
        "        x00=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x00[:,0]=0.0\n",
        "        x00_tensor=Variable(torch.from_numpy(x00),requires_grad=True).to(device)\n",
        "\n",
        "        x01=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x01[:,0]=1.0\n",
        "        x01_tensor=Variable(torch.from_numpy(x01),requires_grad=True).to(device)\n",
        "\n",
        "        x10=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x10[:,1]=0.0\n",
        "        x10_tensor=Variable(torch.from_numpy(x10),requires_grad=True).to(device)\n",
        "\n",
        "        x11=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x11[:,1]=1.0\n",
        "        x11_tensor=Variable(torch.from_numpy(x11),requires_grad=True).to(device)\n",
        "\n",
        "        x20=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x20[:,2]=0.0\n",
        "        x20_tensor=Variable(torch.from_numpy(x20),requires_grad=True).to(device)\n",
        "\n",
        "        x21=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x21[:,2]=1.0\n",
        "        x21_tensor=Variable(torch.from_numpy(x21),requires_grad=True).to(device)\n",
        "\n",
        "        x30=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x30[:,3]=0.0\n",
        "        x30_tensor=Variable(torch.from_numpy(x30),requires_grad=True).to(device)\n",
        "\n",
        "        x31=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x31[:,3]=1.0\n",
        "        x31_tensor=Variable(torch.from_numpy(x31),requires_grad=True).to(device)\n",
        "\n",
        "        phi_00=model(x00_tensor)\n",
        "        phi_01=model(x01_tensor)\n",
        "\n",
        "        phi_10=model(x10_tensor)\n",
        "        phi_11=model(x11_tensor)\n",
        "\n",
        "        phi_20=model(x20_tensor)\n",
        "        phi_21=model(x21_tensor)\n",
        "\n",
        "        phi_30=model(x30_tensor)\n",
        "        phi_31=model(x31_tensor)\n",
        "\n",
        "        phi_p1_np=1.0*np.ones((bound_samp))\n",
        "        phi_m1_np=-1.0*np.ones((bound_samp))\n",
        "\n",
        "        phi_p1=Variable(torch.from_numpy(phi_p1_np),requires_grad=True).to(device)\n",
        "        phi_m1=Variable(torch.from_numpy(phi_m1_np),requires_grad=True).to(device)\n",
        "\n",
        "        loss_bound=torch.mean((phi_00.reshape(-1)-phi_p1)**2)+torch.mean((phi_01.reshape(-1)-phi_p1)**2)+\\\n",
        "        torch.mean((phi_10.reshape(-1)-phi_m1)**2)+torch.mean((phi_11.reshape(-1)-phi_m1)**2)+\\\n",
        "        torch.mean((phi_20.reshape(-1)-phi_p1)**2)+torch.mean((phi_21.reshape(-1)-phi_p1)**2)+\\\n",
        "        torch.mean((phi_30.reshape(-1)-phi_m1)**2)+torch.mean((phi_31.reshape(-1)-phi_m1)**2)\n",
        "\n",
        "\n",
        "\n",
        "        if i<pre_train_step:\n",
        "          loss_res=torch.mean((phi_int_tensor-1)**2)\n",
        "          loss=alpha1*loss_res+alpha5*loss_bound\n",
        "\n",
        "\n",
        "\n",
        "        else:\n",
        "          gradx_all=[]\n",
        "          for j in range(x_dim):\n",
        "            gradx_int = torch.autograd.grad(outputs=phi_int_tensor,inputs=x_all[j],grad_outputs=torch.ones_like(x_tensor[:,j:j+1]),create_graph=True)[0]\n",
        "            gradx_all.append(gradx_int)\n",
        "\n",
        "          gradx2_int=0.0\n",
        "          for j in range(x_dim):\n",
        "            gradx2_j=torch.autograd.grad(outputs=gradx_all[j],inputs=x_all[j],grad_outputs=torch.ones_like(x_tensor[:,j:j+1]),create_graph=True)[0]\n",
        "            gradx2_int=gradx2_int+gradx2_j\n",
        "\n",
        "          loss_res=torch.mean((kappa**2*gradx2_int-(phi_int_tensor**3-phi_int_tensor))**2)\n",
        "\n",
        "          loss = alpha1*loss_res +alpha5*loss_bound\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i) % 50 == 0:\n",
        "            test_sample=10000\n",
        "\n",
        "            xx=np.random.uniform(0.001,0.999,(test_sample,4))\n",
        "\n",
        "            xx_tensor=Variable(torch.from_numpy(xx),requires_grad=True).to(device)\n",
        "\n",
        "            xx_all=[]\n",
        "            for j in range(x_dim):\n",
        "              xx_all.append(xx_tensor[:,j:j+1])\n",
        "\n",
        "            xx_int_tensor=torch.cat(xx_all,1)\n",
        "\n",
        "            phi=model(xx_int_tensor)\n",
        "\n",
        "            gradxx_all=[]\n",
        "            abs_gradxx_phi_2=0.0\n",
        "            for j in range(x_dim):\n",
        "              gradxx_int = torch.autograd.grad(outputs=phi,inputs=xx_all[j],grad_outputs=torch.ones_like(xx_all[j]),create_graph=True)[0]\n",
        "              gradxx_all.append(gradxx_int)\n",
        "\n",
        "\n",
        "            gradxx2_int=0.0\n",
        "            for j in range(x_dim):\n",
        "              gradxx2_j=torch.autograd.grad(outputs=gradxx_all[j],inputs=xx_all[j],grad_outputs=torch.ones_like(xx_all[j]),create_graph=True)[0]\n",
        "              gradxx2_int=gradxx2_int+gradxx2_j\n",
        "\n",
        "\n",
        "            L_phi=(kappa**2*gradxx2_int-(phi**3-phi))**2\n",
        "            L_int=torch.mean(L_phi,0)\n",
        "\n",
        "            lg_batch.append(L_int.item())\n",
        "\n",
        "            if i<pre_train_step:\n",
        "              loss,batch=loss.item(),i\n",
        "              print(\"iteration=\",i,\"loss=\", loss)\n",
        "            else:\n",
        "              loss, batch = alpha1*loss_res.item()+alpha5*loss_bound.item(), i\n",
        "              print(f'batches: {batch+1}')\n",
        "              print(f'loss1: {loss_res} loss_bound: {loss_bound}')\n",
        "\n",
        "            loss_batch.append(loss)\n",
        "            plt_batch.append(i+1)\n",
        "            plt.figure()\n",
        "\n",
        "        if (i) % 500 == 0:\n",
        "\n",
        "            fig_cos_pre(plt_batch,lg_batch,i)\n",
        "            fig_loss_batch_pre(plt_batch,loss_batch)\n",
        "            fig_countour_pre(model,i)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            torch.save(model.state_dict(), model_name)\n",
        "            print(\"Saved PyTorch Model State to \" +str(model_name))\n",
        "\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "    mkdir_pre(all_path+casenum+\"/\")\n",
        "    print(all_path+casenum+\"/\")\n",
        "\n",
        "\n",
        "    model = NeuralNetwork().to(device)\n",
        "    model = model.double()\n",
        "\n",
        "    #model = train_pre(model)\n",
        "    train(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "rcdBqV1V-uO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "zjfjFKLV-22N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pickle\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device to train\")\n",
        "\n",
        "\n",
        "all_path=\"/content/drive/MyDrive/Colab_Notebooks/2023_MEP/0408/\"\n",
        "path_m1=\"/content/drive/MyDrive/Colab_Notebooks/2023_MEP/0408/d4_m2_k005/model_MEP_find_minimals_d4_m2_k005.pth\"\n",
        "path_p1=\"/content/drive/MyDrive/Colab_Notebooks/2023_MEP/0408/d4_p2_k005/model_MEP_find_minimals_d4_p2_k005.pth\"\n",
        "\n",
        "\n",
        "mycase=\"ac_4d_2\"\n",
        "casenum=\"NBC_k008_beta10lg_2\"\n",
        "\n",
        "path_lg=all_path+casenum+\"/\"+\"lgloss\"+\".pkl\"\n",
        "\n",
        "plot_use_all=[]\n",
        "\n",
        "torch.set_printoptions(precision=10)\n",
        "\n",
        "\n",
        "model_name= all_path+casenum+\"/\"+\"model_MEP_\"+mycase+\"_\"+casenum+\".pth\"\n",
        "model_Parameters_name=model_name\n",
        "\n",
        "\n",
        "kappa=0.08\n",
        "load_model = False\n",
        "batches = 100000\n",
        "beta = 10\n",
        "learning_rate = 1e-4\n",
        "dimension = 1\n",
        "dn=28\n",
        "x_dim=4\n",
        "\n",
        "\n",
        "\n",
        "alpha1=1\n",
        "alpha4=0\n",
        "alpha3=0.001\n",
        "alpha5=1\n",
        "\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "  folder = os.path.exists(path)\n",
        "  if not folder:\n",
        "    os.makedirs(path)\n",
        "    print(\"---  new folder...  ---\")\n",
        "    print(\"---  OK  ---\")\n",
        "  else:\n",
        "    print(\"---  There is this folder!  ---\")\n",
        "\n",
        "\n",
        "def fig_loss_batch(plt_batch,loss_batch):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.plot(plt_batch,loss_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "def fig_cos(plt_batch,lg_batch,j):\n",
        "    fig = plt.figure(figsize=(5,4))\n",
        "    plt.plot(plt_batch,lg_batch,'b-')\n",
        "    plt.xlabel(\"Batches\")\n",
        "    plt.ylabel(\"$l_g$\")\n",
        "    if j == batches-500:\n",
        "      plt.savefig(all_path+casenum+\"/\"+\"lgloss\"+\".jpg\")\n",
        "\n",
        "\n",
        "def fig_countour(model,j):\n",
        "    x_plot_use=[0.25,0.5]\n",
        "    t_plot_use=[0,0.25,0.5,0.75,1]\n",
        "\n",
        "    x1 = np.linspace(0.001,0.999,dn,endpoint=True)\n",
        "    x2 = np.linspace(0.001,0.999,dn,endpoint=True)\n",
        "\n",
        "    xx1,xx2=np.meshgrid(x1,x2)\n",
        "    xx1=xx1.reshape((-1,1))\n",
        "    xx2=xx2.reshape((-1,1))\n",
        "\n",
        "    xx=np.hstack((xx1,xx2))\n",
        "\n",
        "    xx1=xx[:,0]\n",
        "    xx2=xx[:,1]\n",
        "\n",
        "    xx1=xx1.reshape(dn,dn)\n",
        "    xx2=xx2.reshape(dn,dn)\n",
        "\n",
        "\n",
        "    for t in t_plot_use:\n",
        "      fig2 = plt.figure(figsize=(6,6))\n",
        "      for k in range(len(x_plot_use)):\n",
        "        for l in range(len(x_plot_use)):\n",
        "          x2=x_plot_use[k]*np.ones((dn*dn,1))\n",
        "          x3=x_plot_use[l]*np.ones((dn*dn,1))\n",
        "          tt=t*np.ones((dn*dn,1))\n",
        "          xxx=np.hstack((xx,x2))\n",
        "          xxxx=np.hstack((xxx,x3))\n",
        "          txxxx=np.hstack((tt,xxxx))\n",
        "          txxxx_tensor=Variable(torch.from_numpy(txxxx),requires_grad=True).to(device)\n",
        "          phi_tensor=model(txxxx_tensor)\n",
        "\n",
        "          phi=phi_tensor.cpu().detach().numpy()\n",
        "          phi_plot_sq=phi.reshape(dn,dn)\n",
        "\n",
        "          fig2.add_subplot(len(x_plot_use),len(x_plot_use),k*len(x_plot_use)+l+1)\n",
        "\n",
        "          picori=phi_plot_sq.reshape(dn,dn)\n",
        "          pic=np.zeros((dn,dn,3))\n",
        "\n",
        "          B=(np.sign(picori)+1)/2*picori\n",
        "          C=(np.sign(picori)-1)/2*picori\n",
        "\n",
        "          pic[:,:,0]=B\n",
        "          pic[:,:,-1]=C\n",
        "\n",
        "          plt.imshow(pic)\n",
        "\n",
        "\n",
        "      plt.savefig(all_path+casenum+\"/\"+\"iteration_\"+str(j)+\"t_\"+str(t)+\".jpg\")\n",
        "\n",
        "class NeuralNetwork_minimum(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork_minimum, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_tanh_stack = nn.Sequential(\n",
        "            nn.Linear(4,64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, s):\n",
        "        phi_pred = self.linear_tanh_stack(s)\n",
        "        return phi_pred\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self,p1,m1):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_tanh_stack = nn.Sequential(\n",
        "            nn.Linear(x_dim+1,100),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(100, 1)\n",
        "        )\n",
        "        self.p1=p1\n",
        "        self.m1=m1\n",
        "\n",
        "\n",
        "    def forward(self, s):\n",
        "        x_pred = self.linear_tanh_stack(s)\n",
        "        ss=s[:,0]\n",
        "        ss=ss.reshape(-1,1)\n",
        "        xx=s[:,1:]\n",
        "\n",
        "        out=ss*(1-ss)*x_pred + (1-ss)*self.p1(xx) + ss*self.m1(xx)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def train(model):\n",
        "    loss_batch=[]\n",
        "    plt_batch=[]\n",
        "    lg_batch=[]\n",
        "\n",
        "    if load_model:\n",
        "        model.load_state_dict(torch.load(model_Parameters_name))\n",
        "\n",
        "    for i in range(batches):\n",
        "\n",
        "        if i<5000:\n",
        "            learning_rate=1e-3\n",
        "        else:\n",
        "            learning_rate=1e-4\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        if i<4000:\n",
        "          alpha1=10\n",
        "          alpha4=1\n",
        "          alpha3=0.001\n",
        "          alpha5=1\n",
        "\n",
        "        else:\n",
        "          alpha1=0\n",
        "          alpha4=1\n",
        "          alpha3=0.001\n",
        "          alpha5=0.01\n",
        "\n",
        "        datanum=50\n",
        "        s_np = np.random.uniform(0.001,0.999,(datanum,1))\n",
        "        x_np = np.random.uniform(0.001,0.999,(datanum,x_dim))\n",
        "\n",
        "        xxind=np.linspace(0,datanum-1,datanum)\n",
        "        s_int_mesh_np,xxind_mesh_np=np.meshgrid(s_np,xxind)\n",
        "\n",
        "        s_int_np=s_int_mesh_np.reshape((-1,1))\n",
        "        xxind_np=xxind_mesh_np.reshape(-1)\n",
        "\n",
        "        x_int_np=x_np[xxind_np.astype(np.int32),:]\n",
        "\n",
        "        s_int_tensor=Variable(torch.from_numpy(s_int_np),requires_grad=True).to(device)\n",
        "        x_int_tensor1=Variable(torch.from_numpy(x_int_np),requires_grad=True).to(device)\n",
        "\n",
        "        x_all=[]\n",
        "        for j in range(x_dim):\n",
        "          x_all.append(x_int_tensor1[:,j:j+1])\n",
        "\n",
        "        x_int_tensor=torch.cat(x_all,1)\n",
        "\n",
        "        sx=torch.cat((s_int_tensor,x_int_tensor),1)\n",
        "\n",
        "        phi_int_tensor=model(sx)\n",
        "\n",
        "        gradx_all=[]\n",
        "        abs_gradx_phi_2=0.0\n",
        "        for j in range(x_dim):\n",
        "          gradx_int = torch.autograd.grad(outputs=phi_int_tensor,inputs=x_all[j],grad_outputs=torch.ones_like(s_int_tensor),create_graph=True)[0]\n",
        "          gradx_all.append(gradx_int)\n",
        "          abs_gradx_phi_2=abs_gradx_phi_2+gradx_int**2\n",
        "\n",
        "        V_int = kappa**2/2*abs_gradx_phi_2+(phi_int_tensor**2-1)**2/4\n",
        "\n",
        "        V_int_mesh=V_int.reshape((datanum,datanum))\n",
        "        F_aint=torch.mean(V_int_mesh,0)\n",
        "\n",
        "\n",
        "        grads_int = torch.autograd.grad(outputs=phi_int_tensor,inputs=s_int_tensor,grad_outputs=torch.ones_like(s_int_tensor),create_graph=True)[0]\n",
        "        grads2_int = torch.autograd.grad(outputs=grads_int,inputs=s_int_tensor,grad_outputs=torch.ones_like(s_int_tensor),create_graph=True)[0]\n",
        "\n",
        "        gradx2_int=0.0\n",
        "        for j in range(x_dim):\n",
        "          gradx2_j=torch.autograd.grad(outputs=gradx_all[j],inputs=x_all[j],grad_outputs=torch.ones_like(s_int_tensor),create_graph=True)[0]\n",
        "          gradx2_int=gradx2_int+gradx2_j\n",
        "\n",
        "\n",
        "        L_phi_mesh_pre=(kappa**2*gradx2_int-(phi_int_tensor**3-phi_int_tensor))**2\n",
        "        L_phi_mesh=L_phi_mesh_pre.reshape((datanum,datanum))\n",
        "        L_int=torch.mean(L_phi_mesh,0)\n",
        "\n",
        "        phi_s_mesh=grads_int.reshape((datanum,datanum))\n",
        "        phi_int=torch.mean(phi_s_mesh**2,0)\n",
        "\n",
        "        loss_1=1/beta*torch.log(torch.mean(torch.exp(beta*F_aint)))\n",
        "        loss_3=torch.mean(torch.abs(grads_int*grads2_int))\n",
        "\n",
        "        loss_4=torch.mean(torch.sqrt(L_int)*torch.sqrt(phi_int))\n",
        "\n",
        "        bound_samp=100\n",
        "\n",
        "        s_b = np.random.uniform(0.001,0.999,(bound_samp,1))\n",
        "        s_b_tensor=Variable(torch.from_numpy(s_b),requires_grad=True).to(device)\n",
        "\n",
        "        x00=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x00[:,0]=0.0\n",
        "        x00_tensor=Variable(torch.from_numpy(x00),requires_grad=True).to(device)\n",
        "        sx00_tensor=torch.cat((s_b_tensor,x00_tensor),1)\n",
        "\n",
        "        x01=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x01[:,0]=1.0\n",
        "        x01_tensor=Variable(torch.from_numpy(x01),requires_grad=True).to(device)\n",
        "        sx01_tensor=torch.cat((s_b_tensor,x01_tensor),1)\n",
        "\n",
        "        x10=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x10[:,1]=0.0\n",
        "        x10_tensor=Variable(torch.from_numpy(x10),requires_grad=True).to(device)\n",
        "        sx10_tensor=torch.cat((s_b_tensor,x10_tensor),1)\n",
        "\n",
        "\n",
        "        x11=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x11[:,1]=1.0\n",
        "        x11_tensor=Variable(torch.from_numpy(x11),requires_grad=True).to(device)\n",
        "        sx11_tensor=torch.cat((s_b_tensor,x11_tensor),1)\n",
        "\n",
        "        x20=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x20[:,2]=0.0\n",
        "        x20_tensor=Variable(torch.from_numpy(x20),requires_grad=True).to(device)\n",
        "        sx20_tensor=torch.cat((s_b_tensor,x20_tensor),1)\n",
        "\n",
        "        x21=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x21[:,2]=1.0\n",
        "        x21_tensor=Variable(torch.from_numpy(x21),requires_grad=True).to(device)\n",
        "        sx21_tensor=torch.cat((s_b_tensor,x21_tensor),1)\n",
        "\n",
        "        x30=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x30[:,3]=0.0\n",
        "        x30_tensor=Variable(torch.from_numpy(x30),requires_grad=True).to(device)\n",
        "        sx30_tensor=torch.cat((s_b_tensor,x30_tensor),1)\n",
        "\n",
        "        x31=np.random.uniform(0.001,0.999,(bound_samp,4))\n",
        "        x31[:,3]=1.0\n",
        "        x31_tensor=Variable(torch.from_numpy(x31),requires_grad=True).to(device)\n",
        "        sx31_tensor=torch.cat((s_b_tensor,x31_tensor),1)\n",
        "\n",
        "        phi_00=model(sx00_tensor)\n",
        "        phi_01=model(sx01_tensor)\n",
        "\n",
        "        phi_10=model(sx10_tensor)\n",
        "        phi_11=model(sx11_tensor)\n",
        "\n",
        "        phi_20=model(sx20_tensor)\n",
        "        phi_21=model(sx21_tensor)\n",
        "\n",
        "        phi_30=model(sx30_tensor)\n",
        "        phi_31=model(sx31_tensor)\n",
        "\n",
        "        phi_p1_np=1.0*np.ones((bound_samp))\n",
        "        phi_m1_np=-1.0*np.ones((bound_samp))\n",
        "\n",
        "        phi_p1=Variable(torch.from_numpy(phi_p1_np),requires_grad=True).to(device)\n",
        "        phi_m1=Variable(torch.from_numpy(phi_m1_np),requires_grad=True).to(device)\n",
        "\n",
        "        loss_bound=torch.mean((phi_00.reshape(-1)-phi_p1)**2)+torch.mean((phi_01.reshape(-1)-phi_p1)**2)+\\\n",
        "        torch.mean((phi_10.reshape(-1)-phi_m1)**2)+torch.mean((phi_11.reshape(-1)-phi_m1)**2)+\\\n",
        "        torch.mean((phi_20.reshape(-1)-phi_p1)**2)+torch.mean((phi_21.reshape(-1)-phi_p1)**2)+\\\n",
        "        torch.mean((phi_30.reshape(-1)-phi_m1)**2)+torch.mean((phi_31.reshape(-1)-phi_m1)**2)\n",
        "\n",
        "\n",
        "        loss = alpha1*loss_1 +alpha4*loss_4+alpha3*loss_3+alpha5*loss_bound\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i) % 50 == 0:\n",
        "            ss_num=5\n",
        "            xx_num=2000\n",
        "\n",
        "            ss = np.linspace(0.001,0.999,ss_num,endpoint=True)\n",
        "            xx=np.random.uniform(0.001,0.999,(xx_num,4))\n",
        "\n",
        "            xxind=np.linspace(0,xx_num-1,xx_num)\n",
        "\n",
        "            ss_int_mesh_np,xxind_int_mesh_np=np.meshgrid(ss,xxind)\n",
        "\n",
        "            ss_int_np=ss_int_mesh_np.reshape((-1,1))\n",
        "            xxind_np=xxind_int_mesh_np.reshape(-1)\n",
        "\n",
        "            xx_int_np=xx[xxind_np.astype(np.int32),:]\n",
        "\n",
        "            ss_int_tensor=Variable(torch.from_numpy(ss_int_np),requires_grad=True).to(device)\n",
        "            xx_int_tensor1=Variable(torch.from_numpy(xx_int_np),requires_grad=True).to(device)\n",
        "\n",
        "            xx_all=[]\n",
        "            for j in range(x_dim):\n",
        "              xx_all.append(xx_int_tensor1[:,j:j+1])\n",
        "\n",
        "            xx_int_tensor=torch.cat(xx_all,1)\n",
        "\n",
        "            ssxx=torch.cat((ss_int_tensor,xx_int_tensor),1)\n",
        "\n",
        "            phi=model(ssxx)\n",
        "\n",
        "            gradxx_all=[]\n",
        "            abs_gradxx_phi_2=0.0\n",
        "            for j in range(x_dim):\n",
        "              gradxx_int = torch.autograd.grad(outputs=phi,inputs=xx_all[j],grad_outputs=torch.ones_like(ss_int_tensor),create_graph=True)[0]\n",
        "              gradxx_all.append(gradxx_int)\n",
        "              abs_gradxx_phi_2=abs_gradxx_phi_2+gradxx_int**2\n",
        "\n",
        "\n",
        "            gradxx2_int=0.0\n",
        "            for j in range(x_dim):\n",
        "              gradxx2_j=torch.autograd.grad(outputs=gradxx_all[j],inputs=xx_all[j],grad_outputs=torch.ones_like(ss_int_tensor),create_graph=True)[0]\n",
        "              gradxx2_int=gradxx2_int+gradxx2_j\n",
        "\n",
        "\n",
        "            L_phi_mesh_pre=(kappa**2*gradxx2_int-(phi**3-phi))**2\n",
        "            L_phi_mesh=L_phi_mesh_pre.reshape((xx_num,ss_num))\n",
        "            L_int=torch.mean(L_phi_mesh,0)\n",
        "\n",
        "            gradss_int = torch.autograd.grad(outputs=phi,inputs=ss_int_tensor,grad_outputs=torch.ones_like(ss_int_tensor),create_graph=True)[0]\n",
        "\n",
        "            phi_ss_mesh=gradss_int.reshape((xx_num,ss_num))\n",
        "            phi_int=torch.mean(phi_ss_mesh**2,0)\n",
        "\n",
        "            lg=torch.mean(torch.sqrt(L_int)*torch.sqrt(phi_int))\n",
        "\n",
        "            lg_batch.append(lg.item())\n",
        "\n",
        "            loss, batch = alpha1*loss_1.item() + alpha4*loss_4.item() + alpha3*loss_3.item()+alpha5*loss_bound.item(), i\n",
        "            print(f'batches: {batch+1}')\n",
        "            print(f'loss1: {alpha1*loss_1} loss4: {alpha4*loss_4} loss3: {alpha3*loss_3} loss_bound: {alpha5*loss_bound}')\n",
        "\n",
        "            loss_batch.append(loss)\n",
        "            plt_batch.append(i+1)\n",
        "            plt.figure()\n",
        "\n",
        "        if (i) % 2000 == 0:\n",
        "\n",
        "            V_int = kappa**2/2*abs_gradxx_phi_2+(phi**2-1)**2/4\n",
        "\n",
        "            V_int_mesh=V_int.reshape((xx_num,ss_num))\n",
        "            F_aint=torch.mean(V_int_mesh,0)\n",
        "            energy_plot=F_aint.cpu().detach().numpy()\n",
        "\n",
        "            fig_cos(plt_batch,lg_batch,i)\n",
        "            fig_loss_batch(plt_batch,loss_batch)\n",
        "            fig_countour(model,i)\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "            torch.save(model.state_dict(), model_name)\n",
        "            print(\"Saved PyTorch Model State to \" +str(model_name))\n",
        "\n",
        "    output = open(path_lg, 'wb')\n",
        "    pickle.dump(lg_batch,output)\n",
        "    output.close()\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "    net_p1=NeuralNetwork_minimum().to(device)\n",
        "    net_p1=net_p1.double()\n",
        "    net_p1.load_state_dict(torch.load(path_p1))\n",
        "\n",
        "    for p in net_p1.parameters():\n",
        "        p.requires_grad=False\n",
        "\n",
        "\n",
        "    net_m1=NeuralNetwork_minimum().to(device)\n",
        "    net_m1=net_m1.double()\n",
        "    net_m1.load_state_dict(torch.load(path_m1))\n",
        "\n",
        "\n",
        "    for p in net_m1.parameters():\n",
        "        p.requires_grad=False\n",
        "\n",
        "\n",
        "    mkdir(all_path+casenum+\"/\")\n",
        "\n",
        "\n",
        "    model = NeuralNetwork(net_p1,net_m1).to(device)\n",
        "    model = model.double()\n",
        "\n",
        "    #model = train_pre(model)\n",
        "    train(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "ka6ZY0xe-4fH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}